{
    "meta": [
        {
            "name": "Name",
            "type": "String"
        },
        {
            "name": "Opportunity",
            "type": "MultiSelect"
        },
        {
            "name": "Contribution Type",
            "type": "MultiSelect"
        },
        {
            "name": "Data Domain",
            "type": "MultiSelect"
        },
        {
            "name": "Dataset Types",
            "type": "MultiSelect"
        },
        {
            "name": "Dataset Generation",
            "type": "MultiSelect"
        },
        {
            "name": "Presentation",
            "type": "MultiSelect"
        },
        {
            "name": "Device",
            "type": "MultiSelect"
        },
        {
            "name": "Input",
            "type": "MultiSelect"
        },
        {
            "name": "Environment",
            "type": "MultiSelect"
        },
        {
            "name": "Space",
            "type": "MultiSelect"
        },
        {
            "name": "Embodiment",
            "type": "MultiSelect"
        },
        {
            "name": "Collaboration ",
            "type": "MultiSelect"
        },
        {
            "name": "Interaction",
            "type": "MultiSelect"
        },
        {
            "name": "Visualization",
            "type": "MultiSelect"
        },
        {
            "name": "Abstract/Natural",
            "type": "MultiSelect"
        },
        {
            "name": "Manipulate",
            "type": "MultiSelect"
        },
        {
            "name": "Position",
            "type": "MultiSelect"
        },
        {
            "name": "Scale",
            "type": "MultiSelect"
        },
        {
            "name": "2D or 3D",
            "type": "MultiSelect"
        },
        {
            "name": "Year",
            "type": "Timeline"
        },
        {
            "name": "Bibtex",
            "type": "String"
        },
        {
            "name": "DOI",
            "type": "String"
        },
        {
            "name": "Authors",
            "type": "MultiSelect"
        }
    ],
    "data": [
        {
            "Name": "\"Tilt Map: Interactive Transitions Between Choropleth Map, Prism Map and Bar Chart in Immersive Environments\"",
            "Opportunity": [
                "Embodied Data Exploration"
            ],
            "Contribution Type": [
                "Evaluation"
            ],
            "Data Domain": [
                "General"
            ],
            "Dataset Types": [
                "Tabular",
                "Spatial"
            ],
            "Dataset Generation": [
                "Static",
                "Pre-computation"
            ],
            "Presentation": [
                "VR",
                "HMD"
            ],
            "Device": [
                "HTC Vive pro"
            ],
            "Input": [
                "Commercial Controller"
            ],
            "Environment": [
                "Room Scale"
            ],
            "Space": [
                "Finite"
            ],
            "Embodiment": [
                "Egocentric"
            ],
            "Collaboration ": [
                "None"
            ],
            "Interaction": [
                "Grab",
                "Move"
            ],
            "Visualization": [
                "2D Bar Chart",
                "2D Map",
                "3D Map"
            ],
            "Abstract/Natural": [
                "Abstract",
                "Natural Spatial Mapping"
            ],
            "Manipulate": [
                "Change",
                "Rotate"
            ],
            "Position": [
                "Unfixed"
            ],
            "Scale": [
                "Medium"
            ],
            "2D or 3D": [
                "2D",
                "3D"
            ],
            "Year": "2021",
            "Bibtex": "@ARTICLE{9123548,\n  author={Yang, Yalong and Dwyer, Tim and Marriott, Kim and Jenny, Bernhard and Goodwin, Sarah},\n  journal={IEEE Transactions on Visualization and Computer Graphics}, \n  title={Tilt Map: Interactive Transitions Between Choropleth Map, Prism Map and Bar Chart in Immersive Environments}, \n  year={2021},\n  volume={27},\n  number={12},\n  pages={4507-4519},\n  doi={10.1109/TVCG.2020.3004137}}",
            "DOI": "10.1109/TVCG.2020.3004137",
            "Authors": [
                "Y. Yang; T. Dwyer; K. Marriott; B. Jenny; S. Goodwin"
            ]
        },
        {
            "Name": "\"Towards Sailing supported by Augmented Reality: Motivation, Methodology and Perspectives\"",
            "Opportunity": [
                "Situated Analytics"
            ],
            "Contribution Type": [
                "Design Study"
            ],
            "Data Domain": [
                "PNT"
            ],
            "Dataset Types": [
                "Spatial"
            ],
            "Dataset Generation": [
                "Live-computation",
                "Dynamic"
            ],
            "Presentation": [
                "AR",
                "HMD"
            ],
            "Device": [
                "Hololens 2"
            ],
            "Input": [
                "Custom Controller"
            ],
            "Environment": [
                "Outdoors"
            ],
            "Space": [
                "Situated"
            ],
            "Embodiment": [
                "Egocentric"
            ],
            "Collaboration ": [
                "None"
            ],
            "Interaction": [
                "Move"
            ],
            "Visualization": [
                "2D Radial Chart"
            ],
            "Abstract/Natural": [
                "Abstract"
            ],
            "Manipulate": [
                "Navigate"
            ],
            "Position": [
                "Fixed (Object)"
            ],
            "Scale": [
                "Medium"
            ],
            "2D or 3D": [
                "2D"
            ],
            "Year": "2020",
            "Bibtex": "'@INPROCEEDINGS{9288459,\n  author={Laera, Francesco. and Foglia, Mario M.. and Evangelista, Alessandro. and Boccaccio, Antonio. and Gattullo, Michele. and Manghisi, Vito M.. and Gabbard, Joseph L.. and Uva, Antonio E.. and Fiorentino, Michele.},\n  booktitle={2020 IEEE International Symposium on Mixed and Augmented Reality Adjunct (ISMAR-Adjunct)}, \n  title={Towards Sailing supported by Augmented Reality: Motivation, Methodology and Perspectives}, \n  year={2020},\n  volume={},\n  number={},\n  pages={269-274},\n  doi={10.1109/ISMAR-Adjunct51615.2020.00076}}",
            "DOI": "10.1109/ISMAR-Adjunct51615.2020.00076",
            "Authors": [
                "F. Laera; M. M. Foglia; A. Evangelista; A. Boccaccio; M. Gattullo; V. M. Manghisi; J. L. Gabbard; A. E. Uva; M. Fiorentino"
            ]
        },
        {
            "Name": "[POSTER] HoloBee: Augmented Reality Based Bee Drift Analysis",
            "Opportunity": [
                "Embodied Data Exploration"
            ],
            "Contribution Type": [
                "Design Study"
            ],
            "Data Domain": [
                "Ecology",
                "Biology",
                "Life sciences"
            ],
            "Dataset Types": [
                "Spatial",
                "Network"
            ],
            "Dataset Generation": [
                "Static",
                "Pre-computation"
            ],
            "Presentation": [
                "HMD",
                "AR"
            ],
            "Device": [
                "Hololens"
            ],
            "Input": [
                "Voice",
                "Gaze",
                "Gesture"
            ],
            "Environment": [
                "Seated",
                "Table"
            ],
            "Space": [
                "Situated"
            ],
            "Embodiment": [
                "Egocentric"
            ],
            "Collaboration ": [
                "None"
            ],
            "Interaction": [
                "Gesture",
                "Gaze",
                "Pinch",
                "Speak"
            ],
            "Visualization": [
                "Area Chart",
                "3D Origin Destination",
                "3D Map"
            ],
            "Abstract/Natural": [
                "Natural Spatial Mapping"
            ],
            "Manipulate": [
                "Translate",
                "Zoom",
                "Rotate",
                "Select"
            ],
            "Position": [
                "Fixed (Point)"
            ],
            "Scale": [
                "Medium"
            ],
            "2D or 3D": [
                "3D"
            ],
            "Year": "2017",
            "Bibtex": "'@INPROCEEDINGS{8088455,\n  author={Nguyen, Huyen and Ketchell, Sarah and Engelke, Ulrich and Thomas, Bruce and Souza, Paulo de},\n  booktitle={2017 IEEE International Symposium on Mixed and Augmented Reality (ISMAR-Adjunct)}, \n  title={[POSTER] HoloBee: Augmented Reality Based Bee Drift Analysis}, \n  year={2017},\n  volume={},\n  number={},\n  pages={87-92},\n  doi={10.1109/ISMAR-Adjunct.2017.38}}",
            "DOI": "10.1109/ISMAR-Adjunct.2017.38",
            "Authors": [
                "H. Nguyen; S. Ketchell; U. Engelke; B. Thomas; P. d. Souza"
            ]
        },
        {
            "Name": "[Poster] Visualization of solar radiation data in augmented reality",
            "Opportunity": [
                "Situated Analytics"
            ],
            "Contribution Type": [
                "System"
            ],
            "Data Domain": [
                "Physical Sciences"
            ],
            "Dataset Types": [
                "Spatial"
            ],
            "Dataset Generation": [
                "Pre-computation",
                "Static"
            ],
            "Presentation": [
                "AR",
                "Tablet",
                "Handheld"
            ],
            "Device": [
                "NA"
            ],
            "Input": [
                "Mobile Device"
            ],
            "Environment": [
                "Outdoors"
            ],
            "Space": [
                "Situated"
            ],
            "Embodiment": [
                "Exocentric"
            ],
            "Collaboration ": [
                "None"
            ],
            "Interaction": [
                "Touch",
                "Move"
            ],
            "Visualization": [
                "3D Matrix/Heatmap"
            ],
            "Abstract/Natural": [
                "Natural Spatial Mapping"
            ],
            "Manipulate": [
                "Change",
                "Navigate"
            ],
            "Position": [
                "Fixed (Point)"
            ],
            "Scale": [
                "Large"
            ],
            "2D or 3D": [
                "3D"
            ],
            "Year": "2014",
            "Bibtex": "'@INPROCEEDINGS{6948437,\n  author={Beatriz Carmo, Maria and Cl\u00e1udio, Ana Paula and Ferreira, Ant\u00f3nio and Afonso, Ana Paula and Redweik, Paula and Catita, Cristina and Brito, Miguel Centeno and Pedrosa, Jos\u00e9 Nunes},\n  booktitle={2014 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, \n  title={[Poster] Visualization of solar radiation data in augmented reality}, \n  year={2014},\n  volume={},\n  number={},\n  pages={255-256},\n  doi={10.1109/ISMAR.2014.6948437}}",
            "DOI": "10.1109/ISMAR.2014.6948437",
            "Authors": [
                "M. Beatriz Carmo; A. P. Cl\u00e1udio; A. Ferreira; A. P. Afonso; P. Redweik; C. Catita; M. C. Brito; J. N. Pedrosa"
            ]
        },
        {
            "Name": "3D Volume Visualization and Screen-based Interaction with Dynamic Ray Casting on Autostereoscopic Display",
            "Opportunity": [
                "Embodied Data Exploration"
            ],
            "Contribution Type": [
                "Technique"
            ],
            "Data Domain": [
                "Biology",
                "Life sciences"
            ],
            "Dataset Types": [
                "Spatial"
            ],
            "Dataset Generation": [
                "Static",
                "Pre-computation"
            ],
            "Presentation": [
                "AR",
                "Tablet",
                "Handheld"
            ],
            "Device": [
                "Custom"
            ],
            "Input": [
                "Custom Controller"
            ],
            "Environment": [
                "Room Scale"
            ],
            "Space": [
                "Finite"
            ],
            "Embodiment": [
                "Exocentric"
            ],
            "Collaboration ": [
                "None"
            ],
            "Interaction": [
                "Move"
            ],
            "Visualization": [
                "Volume Visualization",
                "Anatomical"
            ],
            "Abstract/Natural": [
                "Natural Spatial Mapping"
            ],
            "Manipulate": [
                "Slice",
                "Navigate"
            ],
            "Position": [
                "Fixed (Point)"
            ],
            "Scale": [
                "Small",
                "Medium"
            ],
            "2D or 3D": [
                "3D"
            ],
            "Year": "2021",
            "Bibtex": "'@INPROCEEDINGS{9585840,\n  author={Li, Ruiyang and Huang, Tianqi and Liang, Hanying and Han, Boxuan and Zhang, Xinran and Liao, Hongen},\n  booktitle={2021 IEEE International Symposium on Mixed and Augmented Reality Adjunct (ISMAR-Adjunct)}, \n  title={3D Volume Visualization and Screen-based Interaction with Dynamic Ray Casting on Autostereoscopic Display}, \n  year={2021},\n  volume={},\n  number={},\n  pages={240-245},\n  doi={10.1109/ISMAR-Adjunct54149.2021.00056}}",
            "DOI": "10.1109/ISMAR-Adjunct54149.2021.00056",
            "Authors": [
                "R. Li; T. Huang; H. Liang; B. Han; X. Zhang; H. Liao"
            ]
        },
        {
            "Name": "A Lightweight Tangible 3D Interface for Interactive Visualization of Thin Fiber Structures",
            "Opportunity": [
                "Embodied Data Exploration"
            ],
            "Contribution Type": [
                "Technique"
            ],
            "Data Domain": [
                "Biology",
                "Life sciences"
            ],
            "Dataset Types": [
                "Volumetric data"
            ],
            "Dataset Generation": [
                "Static",
                "Pre-computation"
            ],
            "Presentation": [
                "VR",
                "HMD",
                "Desktop"
            ],
            "Device": [
                "NA"
            ],
            "Input": [
                "Custom Controller"
            ],
            "Environment": [
                "Room Scale"
            ],
            "Space": [
                "Finite"
            ],
            "Embodiment": [
                "Egocentric"
            ],
            "Collaboration ": [
                "None"
            ],
            "Interaction": [
                "Slide",
                "Gesture",
                "Rotate"
            ],
            "Visualization": [
                "Anatomical",
                "3D Flow",
                "Volume Visualization"
            ],
            "Abstract/Natural": [
                "Natural Spatial Mapping"
            ],
            "Manipulate": [
                "Zoom",
                "Slice",
                "Rotate"
            ],
            "Position": [
                "Fixed (Point)"
            ],
            "Scale": [
                "Medium"
            ],
            "2D or 3D": [
                "3D"
            ],
            "Year": "2013",
            "Bibtex": "@ARTICLE{6651934,\n  author={Jackson, Bret and Lau, Tung Yuen and Schroeder, David and Toussaint, Kimani C. and Keefe, Daniel F.},\n  journal={IEEE Transactions on Visualization and Computer Graphics}, \n  title={A Lightweight Tangible 3D Interface for Interactive Visualization of Thin Fiber Structures}, \n  year={2013},\n  volume={19},\n  number={12},\n  pages={2802-2809},\n  doi={10.1109/TVCG.2013.121}}",
            "DOI": "10.1109/TVCG.2013.121",
            "Authors": [
                "B. Jackson; T. Y. Lau; D. Schroeder; K. C. Toussaint; D. F. Keefe"
            ]
        },
        {
            "Name": "Affinity Lens: Data-Assisted Affinity Diagramming with Augmented Reality",
            "Opportunity": [
                "Situated Analytics"
            ],
            "Contribution Type": [
                "System"
            ],
            "Data Domain": [
                "General"
            ],
            "Dataset Types": [
                "Tabular"
            ],
            "Dataset Generation": [
                "Dynamic",
                "Live-computation",
                "Interactive"
            ],
            "Presentation": [
                "AR",
                "Phone",
                "Handheld"
            ],
            "Device": [
                "Android Mobile Device"
            ],
            "Input": [
                "Mobile Device"
            ],
            "Environment": [
                "Wall",
                "Room Scale"
            ],
            "Space": [
                "Situated"
            ],
            "Embodiment": [
                "Exocentric"
            ],
            "Collaboration ": [
                "None"
            ],
            "Interaction": [
                "Touch",
                "Move",
                "Brushing"
            ],
            "Visualization": [
                "2D Bar Chart",
                "2D Line Chart",
                "2D Word Cloud"
            ],
            "Abstract/Natural": [
                "Abstract"
            ],
            "Manipulate": [
                "Translate",
                "Arrange",
                "Select"
            ],
            "Position": [
                "Fixed (Object)"
            ],
            "Scale": [
                "Small"
            ],
            "2D or 3D": [
                "2D"
            ],
            "Year": "2019",
            "Bibtex": "'@inproceedings{10.1145/3290605.3300628,\nauthor = {Subramonyam, Hariharan and Drucker, Steven M. and Adar, Eytan},\ntitle = {Affinity Lens: Data-Assisted Affinity Diagramming with Augmented Reality},\nyear = {2019},\nisbn = {9781450359702},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3290605.3300628},\ndoi = {10.1145/3290605.3300628},\nabstract = {Despite the availability of software to support Affinity Diagramming (AD), practitioners still largely favor physical sticky-notes. Physical notes are easy to set-up, can be moved around in space and offer flexibility when clustering un-structured data. However, when working with mixed data sources such as surveys, designers often trade off the physicality of notes for analytical power. We propose AffinityLens, a mobile-based augmented reality (AR) application for Data-Assisted Affinity Diagramming (DAAD). Our application provides just-in-time quantitative insights overlaid on physical notes. Affinity Lens uses several different types of AR overlays (called lenses) to help users find specific notes, cluster information, and summarize insights from clusters. Through a formative study of AD users, we developed design principles for data-assisted AD and an initial collection of lenses. Based on our prototype, we find that Affinity Lens supports easy switching between qualitative and quantitative 'views' of data, without surrendering the lightweight benefits of existing AD practice.},\nbooktitle = {Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems},\npages = {1\u201313},\nnumpages = {13},\nkeywords = {affinity diagramming, augmented reality, visual analytics},\nlocation = {Glasgow, Scotland Uk},\nseries = {CHI '19}\n}",
            "DOI": "10.1145/3290605.3300628",
            "Authors": [
                "Subramonyam",
                "Hariharan and Drucker",
                "Steven M. and Adar",
                "Eytan"
            ]
        },
        {
            "Name": "Augmented Chironomia for Presenting Data to Remote Audiences",
            "Opportunity": [
                "Engagement"
            ],
            "Contribution Type": [
                "Technique"
            ],
            "Data Domain": [
                "General"
            ],
            "Dataset Types": [
                "Tabular"
            ],
            "Dataset Generation": [
                "Static",
                "Pre-computation"
            ],
            "Presentation": [
                "Desktop"
            ],
            "Device": [
                "NA"
            ],
            "Input": [
                "Gesture"
            ],
            "Environment": [
                "Room Scale"
            ],
            "Space": [
                "Finite"
            ],
            "Embodiment": [
                "Egocentric"
            ],
            "Collaboration ": [
                "None"
            ],
            "Interaction": [
                "Gesture",
                "Grab",
                "Touch"
            ],
            "Visualization": [
                "2D Bar Chart",
                "2D Line Chart",
                "2D Pie Chart"
            ],
            "Abstract/Natural": [
                "Abstract"
            ],
            "Manipulate": [
                "Change",
                "Filter",
                "Translate",
                "Select"
            ],
            "Position": [
                "Unfixed"
            ],
            "Scale": [
                "Medium",
                "Small"
            ],
            "2D or 3D": [
                "2D"
            ],
            "Year": "2022",
            "Bibtex": "@inproceedings{10.1145/3526113.3545614,\nauthor = {Hall, Brian D. and Bartram, Lyn and Brehmer, Matthew},\ntitle = {Augmented Chironomia for Presenting Data to Remote Audiences},\nyear = {2022},\nisbn = {9781450393201},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3526113.3545614},\ndoi = {10.1145/3526113.3545614},\nabstract = {To facilitate engaging and nuanced conversations around data, we contribute a touchless approach to interacting directly with visualization in remote presentations. We combine dynamic charts overlaid on a presenter\u2019s webcam feed with continuous bimanual hand tracking, demonstrating interactions that highlight and manipulate chart elements appearing in the foreground. These interactions are simultaneously functional and deictic, and some allow for the addition of \u201crhetorical flourish\u201d, or expressive movement used when speaking about quantities, categories, and time intervals. We evaluated our approach in two studies with professionals who routinely deliver and attend presentations about data. The first study considered the presenter perspective, where 12 participants delivered presentations to a remote audience using a presentation environment incorporating our approach. The second study considered the audience experience of 17 participants who attended presentations supported by our environment. Finally, we reflect on observations from these studies and discuss related implications for engaging remote audiences in conversations about data.},\nbooktitle = {Proceedings of the 35th Annual ACM Symposium on User Interface Software and Technology},\narticleno = {18},\nnumpages = {14},\nkeywords = {pointing, gesture, augmented reality, video, Visualization, rhetoric},\nlocation = {Bend, OR, USA},\nseries = {UIST '22}\n}",
            "DOI": "10.1145/3526113.3545614",
            "Authors": [
                "Hall",
                "Brian D. and Bartram",
                "Lyn and Brehmer",
                "Matthew"
            ]
        },
        {
            "Name": "Augmented Scale Models: Presenting Multivariate Data Around Physical Scale Models in Augmented Reality",
            "Opportunity": [
                "Situated Analytics"
            ],
            "Contribution Type": [
                "Evaluation"
            ],
            "Data Domain": [
                "General"
            ],
            "Dataset Types": [
                "Tabular"
            ],
            "Dataset Generation": [
                "Static",
                "Pre-computation"
            ],
            "Presentation": [
                "HMD",
                "AR"
            ],
            "Device": [
                "Hololens"
            ],
            "Input": [
                "Gesture"
            ],
            "Environment": [
                "Table"
            ],
            "Space": [
                "Situated"
            ],
            "Embodiment": [
                "Exocentric"
            ],
            "Collaboration ": [
                "None"
            ],
            "Interaction": [
                "Pinch",
                "Touch"
            ],
            "Visualization": [
                "2D Line Chart",
                "2D Bar Chart",
                "2D Scatter Plot"
            ],
            "Abstract/Natural": [
                "Abstract"
            ],
            "Manipulate": [
                "Select",
                "Translate",
                "Arrange"
            ],
            "Position": [
                "Fixed (Point)",
                "Fixed (Object)"
            ],
            "Scale": [
                "Small"
            ],
            "2D or 3D": [
                "2D"
            ],
            "Year": "2022",
            "Bibtex": "'@INPROCEEDINGS{9995479,\n  author={Satriadi, Kadek Ananta and Cunningham, Andrew and Thomas, Bruce H. and Drogemuller, Adam and Odi, Antoine and Patel, Niki and Aston, Cathlyn and Smith, Ross T.},\n  booktitle={2022 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, \n  title={Augmented Scale Models: Presenting Multivariate Data Around Physical Scale Models in Augmented Reality}, \n  year={2022},\n  volume={},\n  number={},\n  pages={54-63},\n  doi={10.1109/ISMAR55827.2022.00019}}",
            "DOI": "10.1109/ISMAR55827.2022.00019",
            "Authors": [
                "Satriadi",
                "Kadek Ananta and Cunningham",
                "Andrew and Thomas",
                "Bruce H. and Drogemuller",
                "Adam and Odi",
                "Antoine and Patel",
                "Niki and Aston",
                "Cathlyn and Smith",
                "Ross T."
            ]
        },
        {
            "Name": "Augmenting Static Visualizations with PapARVis Designer",
            "Opportunity": [
                "Situated Analytics"
            ],
            "Contribution Type": [
                "System"
            ],
            "Data Domain": [
                "General"
            ],
            "Dataset Types": [
                "Tabular",
                "Network"
            ],
            "Dataset Generation": [
                "Static"
            ],
            "Presentation": [
                "AR",
                "Phone",
                "Handheld"
            ],
            "Device": [
                "iPhone 8 Plus"
            ],
            "Input": [
                "Mobile Device"
            ],
            "Environment": [
                "Room Scale",
                "Wall"
            ],
            "Space": [
                "Situated"
            ],
            "Embodiment": [
                "Exocentric"
            ],
            "Collaboration ": [
                "None"
            ],
            "Interaction": [
                "Gesture",
                "Touch"
            ],
            "Visualization": [
                "2D Boxplot",
                "2D Line Chart",
                "Area Chart",
                "2D Sunburst Chart",
                "2D Matrix/Heatmap"
            ],
            "Abstract/Natural": [
                "Abstract"
            ],
            "Manipulate": [
                "Select",
                "Filter",
                "Pan"
            ],
            "Position": [
                "Fixed (Point)"
            ],
            "Scale": [
                "Small",
                "Medium",
                "Large"
            ],
            "2D or 3D": [
                "2D"
            ],
            "Year": "2020",
            "Bibtex": "'@inproceedings{10.1145/3313831.3376436,\nauthor = {Chen, Zhutian and Tong, Wai and Wang, Qianwen and Bach, Benjamin and Qu, Huamin},\ntitle = {Augmenting Static Visualizations with PapARVis Designer},\nyear = {2020},\nisbn = {9781450367080},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3313831.3376436},\ndoi = {10.1145/3313831.3376436},\nabstract = {This paper presents an authoring environment for augmenting static visualizations with virtual content in augmented reality.Augmenting static visualizations can leverage the best of both physical and digital worlds, but its creation currently involves different tools and devices, without any means to explicitly design and debug both static and virtual content simultaneously. To address these issues, we design an environment that seamlessly integrates all steps of a design and deployment workflow through its main features: i) an extension to Vega, ii) a preview, and iii) debug hints that facilitate valid combinations of static and augmented content. We inform our design through a design space with four ways to augment static visualizations. We demonstrate the expressiveness of our tool through examples, including books, posters, projections, wall-sized visualizations. A user study shows high user satisfaction of our environment and confirms that participants can create augmented visualizations in an average of 4.63 minutes.},\nbooktitle = {Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems},\npages = {1\u201312},\nnumpages = {12},\nkeywords = {visualization in augmented reality, data visualization authoring, augmented static visualization},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '20}\n}",
            "DOI": "10.1145/3411763.3451588",
            "Authors": [
                "Chen",
                "Zhutian and Tong",
                "Wai and Wang",
                "Qianwen and Bach",
                "Benjamin and Qu",
                "Huamin"
            ]
        },
        {
            "Name": "Data Abstraction for Visual and Haptic Representations in Flow Visualization",
            "Opportunity": [
                "Multi-Sensory Presentation"
            ],
            "Contribution Type": [
                "Technique"
            ],
            "Data Domain": [
                "Material Science",
                "Physical Sciences"
            ],
            "Dataset Types": [
                "Fields"
            ],
            "Dataset Generation": [
                "Static",
                "Pre-computation"
            ],
            "Presentation": [
                "HMD",
                "VR",
                "Haptic Display"
            ],
            "Device": [
                "Oculus Quest 2"
            ],
            "Input": [
                "Gesture"
            ],
            "Environment": [
                "Seated",
                "Table"
            ],
            "Space": [
                "Infinite"
            ],
            "Embodiment": [
                "Egocentric"
            ],
            "Collaboration ": [
                "None"
            ],
            "Interaction": [
                "Move",
                "Grab"
            ],
            "Visualization": [
                "Volume Visualization"
            ],
            "Abstract/Natural": [
                "Natural Spatial Mapping"
            ],
            "Manipulate": [
                "Translate",
                "Rotate",
                "Scale"
            ],
            "Position": [
                "Unfixed"
            ],
            "Scale": [
                "Large",
                "Small"
            ],
            "2D or 3D": [
                "3D"
            ],
            "Year": "2022",
            "Bibtex": "'@inproceedings{10.1145/3562939.3565651,\nauthor = {Bhardwaj, Ayush and Kang, Sungjoo and Kim, Jin Ryong},\ntitle = {Data Abstraction for Visual and Haptic Representations in Flow Visualization},\nyear = {2022},\nisbn = {9781450398893},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3562939.3565651},\ndoi = {10.1145/3562939.3565651},\nabstract = {This paper presents a new way of data abstraction for visual and haptic representations in immersive analytics using a mid-air haptic display. Visual and haptic abstraction is proposed to transform raw data (wind tunnel data) into another form of data for effective visual and haptic data mapping. Three main features are extracted: (i) Magnitude of Velocity, (ii) Recirculation Region, and (iii) Vorticity. For each feature, visual and haptic abstractions are defined based on data characterization and data reduction. A preliminary study shows a promising direction toward multimodal data interaction in immersive analytics.},\nbooktitle = {Proceedings of the 28th ACM Symposium on Virtual Reality Software and Technology},\narticleno = {50},\nnumpages = {2},\nkeywords = {Immersive analytics, haptic, interactive data visualization, data abstraction, data transformation},\nlocation = {Tsukuba, Japan},\nseries = {VRST '22}\n}",
            "DOI": "10.1145/3562939.3565651",
            "Authors": [
                "Bhardwaj",
                "Ayush and Kang",
                "Sungjoo and Kim",
                "Jin Ryong"
            ]
        },
        {
            "Name": "Data Visceralization: Enabling Deeper Understanding of Data Using Virtual Reality",
            "Opportunity": [
                "Engagement"
            ],
            "Contribution Type": [
                "Design Study"
            ],
            "Data Domain": [
                "General",
                "Sports"
            ],
            "Dataset Types": [
                "Tabular"
            ],
            "Dataset Generation": [
                "Static",
                "Pre-computation"
            ],
            "Presentation": [
                "VR",
                "HMD"
            ],
            "Device": [
                "NA"
            ],
            "Input": [
                "None"
            ],
            "Environment": [
                "Room Scale"
            ],
            "Space": [
                "Infinite"
            ],
            "Embodiment": [
                "Exocentric"
            ],
            "Collaboration ": [
                "None"
            ],
            "Interaction": [
                "None"
            ],
            "Visualization": [
                "Glyph/Realism"
            ],
            "Abstract/Natural": [
                "Natural Spatial Mapping"
            ],
            "Manipulate": [
                "Navigate"
            ],
            "Position": [
                "Unfixed"
            ],
            "Scale": [
                "Large",
                "Medium"
            ],
            "2D or 3D": [
                "3D"
            ],
            "Year": "2021",
            "Bibtex": "@ARTICLE{9229242,\n  author={Lee, Benjamin and Brown, Dave and Lee, Bongshin and Hurter, Christophe and Drucker, Steven and Dwyer, Tim},\n  journal={IEEE Transactions on Visualization and Computer Graphics}, \n  title={Data Visceralization: Enabling Deeper Understanding of Data Using Virtual Reality}, \n  year={2021},\n  volume={27},\n  number={2},\n  pages={1095-1105},\n  doi={10.1109/TVCG.2020.3030435}}",
            "DOI": "10.1109/TVCG.2020.3030435",
            "Authors": [
                "B. Lee; D. Brown; B. Lee; C. Hurter; S. Drucker; T. Dwyer"
            ]
        },
        {
            "Name": "DataHop: Spatial Data Exploration in Virtual Reality",
            "Opportunity": [
                "Spatial Immersion"
            ],
            "Contribution Type": [
                "System"
            ],
            "Data Domain": [
                "General"
            ],
            "Dataset Types": [
                "Tabular"
            ],
            "Dataset Generation": [
                "Pre-computation",
                "Static",
                "Live-computation",
                "Interactive"
            ],
            "Presentation": [
                "HMD",
                "VR"
            ],
            "Device": [
                "Oculus Rift S"
            ],
            "Input": [
                "Commercial Controller"
            ],
            "Environment": [
                "Room Scale"
            ],
            "Space": [
                "Infinite"
            ],
            "Embodiment": [
                "Egocentric"
            ],
            "Collaboration ": [
                "None"
            ],
            "Interaction": [
                "Touch",
                "Grab",
                "Raycast",
                "Move",
                "Slide"
            ],
            "Visualization": [
                "2D Histogram",
                "3D Scatter Plot",
                "3D Bubble Chart",
                "2D Node Link"
            ],
            "Abstract/Natural": [
                "Abstract"
            ],
            "Manipulate": [
                "Navigate",
                "Arrange",
                "Change",
                "Filter"
            ],
            "Position": [
                "Fixed (Point)",
                "Fixed (Body)"
            ],
            "Scale": [
                "Small",
                "Medium"
            ],
            "2D or 3D": [
                "3D",
                "2D"
            ],
            "Year": "2020",
            "Bibtex": "'@inproceedings{10.1145/3379337.3415878,\nauthor = {Hayatpur, Devamardeep and Xia, Haijun and Wigdor, Daniel},\ntitle = {DataHop: Spatial Data Exploration in Virtual Reality},\nyear = {2020},\nisbn = {9781450375146},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3379337.3415878},\ndoi = {10.1145/3379337.3415878},\nabstract = {Virtual reality has recently been adopted for use within the domain of visual analytics because it can provide users with an endless workspace within which they can be actively engaged and use their spatial reasoning skills for data analysis. However, virtual worlds need to utilize layouts and organizational schemes that are meaningful to the user and beneficial for data analysis. This paper presents DataHop, a novel visualization system that enables users to lay out their data analysis steps in a virtual environment. With a Filter, a user can specify the modification they wish to perform on one or more input data panels (i.e., containers of points), along with where output data panels should be placed in the virtual environment. Using this simple tool, highly intricate and useful visualizations may be generated and traversed by harnessing a user's spatial abilities. An exploratory study conducted with six virtual reality users evaluated the usability, affordances, and performance of DataHop for data analysis tasks, and found that spatially mapping one's workflow can be beneficial when exploring multidimensional datasets.},\nbooktitle = {Proceedings of the 33rd Annual ACM Symposium on User Interface Software and Technology},\npages = {818\u2013828},\nnumpages = {11},\nkeywords = {immersive data visualization, spatial skills},\nlocation = {Virtual Event, USA},\nseries = {UIST '20}\n}",
            "DOI": "10.1145/3379337.3415878",
            "Authors": [
                "Hayatpur",
                "Devamardeep and Xia",
                "Haijun and Wigdor",
                "Daniel"
            ]
        },
        {
            "Name": "Dataspace: A Reconfigurable Hybrid Reality Environment for Collaborative Information Analysis",
            "Opportunity": [
                "Collaboration"
            ],
            "Contribution Type": [
                "System"
            ],
            "Data Domain": [
                "General"
            ],
            "Dataset Types": [
                "Spatial",
                "Network",
                "Volumetric data",
                "Tabular"
            ],
            "Dataset Generation": [
                "Static",
                "Dynamic",
                "Pre-computation",
                "Live-computation",
                "Interactive"
            ],
            "Presentation": [
                "HMD",
                "AR",
                "LID",
                "Tabletop"
            ],
            "Device": [
                "Hololens"
            ],
            "Input": [
                "Gesture",
                "Gaze",
                "Commercial Controller",
                "Mobile Device",
                "Mouse/Keyboard",
                "Voice"
            ],
            "Environment": [
                "Room Scale",
                "Wall",
                "Table"
            ],
            "Space": [
                "Situated"
            ],
            "Embodiment": [
                "Exocentric"
            ],
            "Collaboration ": [
                "Synchronous Co-located",
                "Synchronous Distributed",
                "Cross-virtuality"
            ],
            "Interaction": [
                "Touch",
                "Pinch",
                "Grab",
                "Gesture",
                "Speak"
            ],
            "Visualization": [
                "2D Line Chart",
                "2D Bar Chart",
                "Volume Visualization",
                "2D Node Link",
                "3D Node Link",
                "2D Trajectory",
                "3D Globe",
                "Anatomical",
                "2D Map"
            ],
            "Abstract/Natural": [
                "Abstract",
                "Natural Spatial Mapping"
            ],
            "Manipulate": [
                "Select",
                "Arrange",
                "Change",
                "Filter"
            ],
            "Position": [
                "Fixed (Point)",
                "Fixed (Object)"
            ],
            "Scale": [
                "Medium"
            ],
            "2D or 3D": [
                "2D",
                "3D"
            ],
            "Year": "2019",
            "Bibtex": "'@INPROCEEDINGS{8797733,\n  author={Cavallo, Marco and Dholakia, Mishal and Havlena, Matous and Ocheltree, Kenneth and Podlaseck, Mark},\n  booktitle={2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, \n  title={Dataspace: A Reconfigurable Hybrid Reality Environment for Collaborative Information Analysis}, \n  year={2019},\n  volume={},\n  number={},\n  pages={145-153},\n  doi={10.1109/VR.2019.8797733}}",
            "DOI": "10.1109/VR.2019.8797733",
            "Authors": [
                "M. Cavallo; M. Dholakia; M. Havlena; K. Ocheltree; M. Podlaseck"
            ]
        },
        {
            "Name": "Dear Pictograph: Investigating the Role of Personalization and Immersion for Consuming and Enjoying Visualizations",
            "Opportunity": [
                "Engagement"
            ],
            "Contribution Type": [
                "Evaluation"
            ],
            "Data Domain": [
                "Personal"
            ],
            "Dataset Types": [
                "Tabular"
            ],
            "Dataset Generation": [
                "Interactive",
                "Dynamic"
            ],
            "Presentation": [
                "HMD",
                "VR",
                "Tablet"
            ],
            "Device": [
                "HP Reverb"
            ],
            "Input": [
                "Commercial Controller"
            ],
            "Environment": [
                "Room Scale"
            ],
            "Space": [
                "Infinite"
            ],
            "Embodiment": [
                "Egocentric"
            ],
            "Collaboration ": [
                "None"
            ],
            "Interaction": [
                "Draw"
            ],
            "Visualization": [
                "3D Pictogram"
            ],
            "Abstract/Natural": [
                "Abstract"
            ],
            "Manipulate": [
                "Change",
                "Select"
            ],
            "Position": [
                "Fixed (Point)"
            ],
            "Scale": [
                "Medium",
                "Large"
            ],
            "2D or 3D": [
                "3D"
            ],
            "Year": "2020",
            "Bibtex": "'@inproceedings{10.1145/3313831.3376348,\nauthor = {Romat, Hugo and Henry Riche, Nathalie and Hurter, Christophe and Drucker, Steven and Amini, Fereshteh and Hinckley, Ken},\ntitle = {Dear Pictograph: Investigating the Role of Personalization and Immersion for Consuming and Enjoying Visualizations},\nyear = {2020},\nisbn = {9781450367080},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3313831.3376348},\ndoi = {10.1145/3313831.3376348},\nabstract = {Much of the visualization literature focuses on assessment of visual representations with regard to their effectiveness for understanding data. In the present work, we instead focus on making data visualization experiences more enjoyable, to foster deeper engagement with data. We investigate two strategies to make visualization experiences more enjoyable and engaging: personalization, and immersion. We selected pictographs (composed of multiple data glyphs) as this representation affords creative freedom, allowing people to craft symbolic or whimsical shapes of personal significance to represent data. We present the results of a qualitative study with 12 participants crafting pictographs using a large pen-enabled device and while immersed within a VR environment. Our results indicate that personalization and immersion both have positive impact on making visualizations more enjoyable experiences.},\nbooktitle = {Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems},\npages = {1\u201313},\nnumpages = {13},\nkeywords = {immersion, qualitative study, personalization, visualization},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '20}\n}",
            "DOI": "10.1145/3313831.3376348",
            "Authors": [
                "Romat",
                "Hugo and Henry Riche",
                "Nathalie and Hurter",
                "Christophe and Drucker",
                "Steven and Amini",
                "Fereshteh and Hinckley",
                "Ken"
            ]
        },
        {
            "Name": "Developing Virtual Reality Visualizations for Unsteady Flow Analysis of Dinosaur Track Formation using Scientific Sketching",
            "Opportunity": [
                "Spatial Immersion"
            ],
            "Contribution Type": [
                "Design Study"
            ],
            "Data Domain": [
                "Paleontology",
                "Physical Sciences"
            ],
            "Dataset Types": [
                "Volumetric data",
                "Fields"
            ],
            "Dataset Generation": [
                "Pre-computation",
                "Dynamic"
            ],
            "Presentation": [
                "LID"
            ],
            "Device": [
                "Custom"
            ],
            "Input": [
                "Commercial Controller"
            ],
            "Environment": [
                "Room Scale",
                "Wall",
                "Table"
            ],
            "Space": [
                "Finite"
            ],
            "Embodiment": [
                "Exocentric"
            ],
            "Collaboration ": [
                "None"
            ],
            "Interaction": [
                "Raycast",
                "Move"
            ],
            "Visualization": [
                "Volume Visualization",
                "3D Flow"
            ],
            "Abstract/Natural": [
                "Natural Spatial Mapping"
            ],
            "Manipulate": [
                "Select",
                "Annotate",
                "Slice"
            ],
            "Position": [
                "Fixed (Point)"
            ],
            "Scale": [
                "Medium"
            ],
            "2D or 3D": [
                "3D"
            ],
            "Year": "2019",
            "Bibtex": "@ARTICLE{8672601,\n  author={Novotny, Johannes and Tveite, Joshua and Turner, Morgan L. and Gatesy, Stephen and Drury, Fritz and Falkingham, Peter and Laidlaw, David H.},\n  journal={IEEE Transactions on Visualization and Computer Graphics}, \n  title={Developing Virtual Reality Visualizations for Unsteady Flow Analysis of Dinosaur Track Formation using Scientific Sketching}, \n  year={2019},\n  volume={25},\n  number={5},\n  pages={2145-2154},\n  doi={10.1109/TVCG.2019.2898796}}",
            "DOI": "10.1109/TVCG.2019.2898796",
            "Authors": [
                "J. Novotny; J. Tveite; M. L. Turner; S. Gatesy; F. Drury; P. Falkingham; D. H. Laidlaw"
            ]
        },
        {
            "Name": "Embodied Axes: Tangible, Actuated Interaction for 3D Augmented Reality Data Spaces",
            "Opportunity": [
                "Embodied Data Exploration"
            ],
            "Contribution Type": [
                "System"
            ],
            "Data Domain": [
                "Biology",
                "Life sciences"
            ],
            "Dataset Types": [
                "Tabular",
                "Spatial"
            ],
            "Dataset Generation": [
                "Static"
            ],
            "Presentation": [
                "AR",
                "HMD"
            ],
            "Device": [
                "Metavision Meta 2 glasses",
                "HTC vive"
            ],
            "Input": [
                "Custom Controller",
                "Gesture",
                "Commercial Controller"
            ],
            "Environment": [
                "Seated",
                "Table"
            ],
            "Space": [
                "Situated"
            ],
            "Embodiment": [
                "Egocentric"
            ],
            "Collaboration ": [
                "Synchronous Distributed"
            ],
            "Interaction": [
                "Grab",
                "Slide",
                "Pinch",
                "Raycast"
            ],
            "Visualization": [
                "3D Bar Chart",
                "3D Scatter Plot",
                "Volume Visualization"
            ],
            "Abstract/Natural": [
                "Abstract",
                "Natural Spatial Mapping"
            ],
            "Manipulate": [
                "Select",
                "Filter",
                "Slice"
            ],
            "Position": [
                "Fixed (Point)",
                "Fixed (Object)"
            ],
            "Scale": [
                "Small"
            ],
            "2D or 3D": [
                "3D"
            ],
            "Year": "2020",
            "Bibtex": "@inproceedings{10.1145/3313831.3376613,\nauthor = {Cordeil, Maxime and Bach, Benjamin and Cunningham, Andrew and Montoya, Bastian and Smith, Ross T. and Thomas, Bruce H. and Dwyer, Tim},\ntitle = {Embodied Axes: Tangible, Actuated Interaction for 3D Augmented Reality Data Spaces},\nyear = {2020},\nisbn = {9781450367080},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3313831.3376613},\ndoi = {10.1145/3313831.3376613},\nabstract = {We present Embodied Axes, a controller which supports selection operations for 3D imagery and data visualisations in Augmented Reality. The device is an embodied representation of a 3D data space -- each of its three orthogonal arms corresponds to a data axis or domain specific frame of reference. Each axis is composed of a pair of tangible, actuated range sliders for precise data selection, and rotary encoding knobs for additional parameter tuning or menu navigation. The motor actuated sliders support alignment to positions of significant values within the data, or coordination with other input: e.g., mid-air gestures in the data space, touch gestures on the surface below the data, or another Embodied Axes device supporting multi-user scenarios. We conducted expert enquiries in medical imaging which provided formative feedback on domain tasks and refinements to the design. Additionally, a controlled user study was performed and found that the Embodied Axes was overall more accurate than conventional tracked controllers for selection tasks.},\nbooktitle = {Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems},\npages = {1\u201312},\nnumpages = {12},\nkeywords = {tangible interaction, augmented reality, 3d visualisation, device, actuation},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '20}\n}",
            "DOI": "10.1145/3313831.3376613",
            "Authors": [
                "Cordeil",
                "Maxime and Bach",
                "Benjamin and Cunningham",
                "Andrew and Montoya",
                "Bastian and Smith",
                "Ross T. and Thomas",
                "Bruce H. and Dwyer",
                "Tim"
            ]
        },
        {
            "Name": "Evaluating an Immersive Space-Time Cube Geovisualization for Intuitive Trajectory Data Exploration",
            "Opportunity": [
                "Spatial Immersion"
            ],
            "Contribution Type": [
                "Evaluation"
            ],
            "Data Domain": [
                "PNT"
            ],
            "Dataset Types": [
                "Spatial"
            ],
            "Dataset Generation": [
                "Static",
                "Pre-computation"
            ],
            "Presentation": [
                "VR",
                "HMD"
            ],
            "Device": [
                "Oculus Rift CV1"
            ],
            "Input": [
                "Gesture"
            ],
            "Environment": [
                "Room Scale",
                "Table"
            ],
            "Space": [
                "Finite"
            ],
            "Embodiment": [
                "Exocentric"
            ],
            "Collaboration ": [
                "None"
            ],
            "Interaction": [
                "Grab",
                "Touch",
                "Raycast"
            ],
            "Visualization": [
                "3D Trajectory",
                "Space-Time Cube",
                "2D Map"
            ],
            "Abstract/Natural": [
                "Abstract",
                "Natural Spatial Mapping"
            ],
            "Manipulate": [
                "Select",
                "Zoom",
                "Change",
                "Pan"
            ],
            "Position": [
                "Fixed (Point)"
            ],
            "Scale": [
                "Medium"
            ],
            "2D or 3D": [
                "3D",
                "2D"
            ],
            "Year": "2020",
            "Bibtex": "@ARTICLE{8854316,\n  author={Filho, Jorge A. Wagner and Stuerzlinger, Wolfgang and Nedel, Luciana},\n  journal={IEEE Transactions on Visualization and Computer Graphics}, \n  title={Evaluating an Immersive Space-Time Cube Geovisualization for Intuitive Trajectory Data Exploration}, \n  year={2020},\n  volume={26},\n  number={1},\n  pages={514-524},\n  doi={10.1109/TVCG.2019.2934415}}",
            "DOI": "10.1109/TVCG.2019.2934415",
            "Authors": [
                "J. A. W. Filho; W. Stuerzlinger; L. Nedel"
            ]
        },
        {
            "Name": "Exploration &amp; Anthropomorphism in Immersive Unit Visualizations",
            "Opportunity": [
                "Engagement"
            ],
            "Contribution Type": [
                "System"
            ],
            "Data Domain": [
                "General"
            ],
            "Dataset Types": [
                "Tabular"
            ],
            "Dataset Generation": [
                "Static",
                "Pre-computation"
            ],
            "Presentation": [
                "VR",
                "HMD"
            ],
            "Device": [
                "HTC vive"
            ],
            "Input": [
                "Commercial Controller"
            ],
            "Environment": [
                "Room Scale"
            ],
            "Space": [
                "Infinite"
            ],
            "Embodiment": [
                "Egocentric"
            ],
            "Collaboration ": [
                "None"
            ],
            "Interaction": [
                "Click"
            ],
            "Visualization": [
                "Glyph/Realism"
            ],
            "Abstract/Natural": [
                "Abstract"
            ],
            "Manipulate": [
                "Zoom",
                "Arrange",
                "Change"
            ],
            "Position": [
                "Fixed (Point)"
            ],
            "Scale": [
                "Large"
            ],
            "2D or 3D": [
                "3D"
            ],
            "Year": "2018",
            "Bibtex": "'@inproceedings{10.1145/3170427.3188544,\nauthor = {Ivanov, Alexander and Danyluk, Kurtis Thorvald and Willett, Wesley},\ntitle = {Exploration &amp; Anthropomorphism in Immersive Unit Visualizations},\nyear = {2018},\nisbn = {9781450356213},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3170427.3188544},\ndoi = {10.1145/3170427.3188544},\nabstract = {We report on an initial examination of the potential of immersive unit visualizations in virtual reality, showing how these visualizations can help viewers examine data at multiple scales and support affective, personal experiences with data. We outline unique opportunities for unit visualizations in virtual reality, including support for (1) dynamic scale transitions, (2)&nbsp;immersive exploration, and (3) anthropomorphic interactions. We then demonstrate a prototype system and discuss the potential for virtual reality visualization to support personal interactions with data.},\nbooktitle = {Extended Abstracts of the 2018 CHI Conference on Human Factors in Computing Systems},\npages = {1\u20136},\nnumpages = {6},\nkeywords = {scale, anthropomorphism, virtual reality, unit visualization, data visualization},\nlocation = {Montreal QC, Canada},\nseries = {CHI EA '18}\n}",
            "DOI": "10.1145/3170427.3188544",
            "Authors": [
                "Ivanov",
                "Alexander and Danyluk",
                "Kurtis Thorvald and Willett",
                "Wesley"
            ]
        },
        {
            "Name": "Exploring Data in Virtual Reality: Comparisons with 2D Data Visualizations",
            "Opportunity": [
                "Spatial Immersion"
            ],
            "Contribution Type": [
                "Evaluation"
            ],
            "Data Domain": [
                "General"
            ],
            "Dataset Types": [
                "Tabular"
            ],
            "Dataset Generation": [
                "Static",
                "Pre-computation"
            ],
            "Presentation": [
                "VR",
                "HMD"
            ],
            "Device": [
                "Daydream"
            ],
            "Input": [
                "Commercial Controller"
            ],
            "Environment": [
                "Room Scale"
            ],
            "Space": [
                "Finite"
            ],
            "Embodiment": [
                "Exocentric"
            ],
            "Collaboration ": [
                "None"
            ],
            "Interaction": [
                "Brushing"
            ],
            "Visualization": [
                "3D Scatter Plot",
                "3D Parallel Coord"
            ],
            "Abstract/Natural": [
                "Abstract"
            ],
            "Manipulate": [
                "Navigate",
                "Select"
            ],
            "Position": [
                "Fixed (Point)"
            ],
            "Scale": [
                "Large"
            ],
            "2D or 3D": [
                "3D"
            ],
            "Year": "2018",
            "Bibtex": "'@inproceedings{10.1145/3170427.3188537,\nauthor = {Millais, Patrick and Jones, Simon L. and Kelly, Ryan},\ntitle = {Exploring Data in Virtual Reality: Comparisons with 2D Data Visualizations},\nyear = {2018},\nisbn = {9781450356213},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3170427.3188537},\ndoi = {10.1145/3170427.3188537},\nabstract = {Virtual Reality (VR) has often been discussed as a promising medium for immersive data visualization and exploration. However, few studies have evaluated users' open-ended exploration of multi-dimensional datasets using VR and compared the results with that of traditional (2D) visualizations. Using a workload- and insight-based evaluation methodology, we conducted a user study to perform such a comparison. We find that there is no overall task-workload difference between traditional visualizations and visualizations in VR, but there are differences in the accuracy and depth of insights that users gain. Our results also suggest that users feel more satisfied and successful when using VR data exploration tools, thus demonstrating the potential of VR as an engaging medium for visual data analytics.},\nbooktitle = {Extended Abstracts of the 2018 CHI Conference on Human Factors in Computing Systems},\npages = {1\u20136},\nnumpages = {6},\nkeywords = {virtual reality, data dashboards, data visualisation},\nlocation = {Montreal QC, Canada},\nseries = {CHI EA '18}\n}",
            "DOI": "10.1145/3170427.3188537",
            "Authors": [
                "Millais",
                "Patrick and Jones",
                "Simon L. and Kelly",
                "Ryan"
            ]
        },
        {
            "Name": "Exploring Parallel Coordinates Plots in Virtual Reality",
            "Opportunity": [
                "Spatial Immersion"
            ],
            "Contribution Type": [
                "Evaluation"
            ],
            "Data Domain": [
                "General"
            ],
            "Dataset Types": [
                "Tabular"
            ],
            "Dataset Generation": [
                "Static",
                "Live-computation"
            ],
            "Presentation": [
                "HMD",
                "VR"
            ],
            "Device": [
                "NA"
            ],
            "Input": [
                "Commercial Controller",
                "Gaze"
            ],
            "Environment": [
                "Room Scale"
            ],
            "Space": [
                "Infinite"
            ],
            "Embodiment": [
                "Exocentric"
            ],
            "Collaboration ": [
                "None"
            ],
            "Interaction": [
                "Gaze"
            ],
            "Visualization": [
                "3D Parallel Coord",
                "3D Scatter Plot"
            ],
            "Abstract/Natural": [
                "Abstract"
            ],
            "Manipulate": [
                "Select",
                "Change",
                "Navigate",
                "Arrange"
            ],
            "Position": [
                "Unfixed"
            ],
            "Scale": [
                "Medium",
                "Large"
            ],
            "2D or 3D": [
                "3D"
            ],
            "Year": "2019",
            "Bibtex": "'@inproceedings{10.1145/3290607.3313068,\nauthor = {Tadeja, Slawomir Konrad and Kipouros, Timoleon and Kristensson, Per Ola},\ntitle = {Exploring Parallel Coordinates Plots in Virtual Reality},\nyear = {2019},\nisbn = {9781450359719},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3290607.3313068},\ndoi = {10.1145/3290607.3313068},\nabstract = {Parallel Coordinates Plots (PCP) are a widely used approach to interactively visualize and analyze multidimensional scientific data in a 2D environment. In this paper, we explore the use of Parallel Coordinates in an immersive Virtual Reality (VR) 3D visualization environment as a means to support the decision-making process in engineering design processes. We evaluate the potential of VR PCP using a formative qualitative study with seven participants. In a task involving 54 points with 29 dimensions per point, we found that participants were able to detect patterns in the dataset compared with a previously published study with two expert users using traditional 2D PCP, which acts as the gold standard for the dataset. The dataset describes the Pareto front for a three-objective aerodynamic design optimization study in turbomachinery.},\nbooktitle = {Extended Abstracts of the 2019 CHI Conference on Human Factors in Computing Systems},\npages = {1\u20136},\nnumpages = {6},\nkeywords = {visual analytics, parallel coordinates, data visualization, virtual reality, decision-making},\nlocation = {Glasgow, Scotland Uk},\nseries = {CHI EA '19}\n}",
            "DOI": "10.1145/3290607.3313068",
            "Authors": [
                "Tadeja",
                "Slawomir Konrad and Kipouros",
                "Timoleon and Kristensson",
                "Per Ola"
            ]
        },
        {
            "Name": "Exploring the Effects of Scale in Augmented Reality-Empowered Visual Analytics",
            "Opportunity": [
                "Spatial Immersion"
            ],
            "Contribution Type": [
                "Evaluation"
            ],
            "Data Domain": [
                "General"
            ],
            "Dataset Types": [
                "Network",
                "Spatial"
            ],
            "Dataset Generation": [
                "Static",
                "Pre-computation"
            ],
            "Presentation": [
                "AR",
                "Tabletop"
            ],
            "Device": [
                "Hololens"
            ],
            "Input": [
                "Gesture"
            ],
            "Environment": [
                "Room Scale",
                "Table"
            ],
            "Space": [
                "Finite"
            ],
            "Embodiment": [
                "Egocentric",
                "Exocentric"
            ],
            "Collaboration ": [
                "None"
            ],
            "Interaction": [
                "Pinch",
                "Move"
            ],
            "Visualization": [
                "3D Node Link"
            ],
            "Abstract/Natural": [
                "Abstract"
            ],
            "Manipulate": [
                "Zoom",
                "Rotate",
                "Scale",
                "Arrange",
                "Select"
            ],
            "Position": [
                "Fixed (Point)"
            ],
            "Scale": [
                "Medium",
                "Large"
            ],
            "2D or 3D": [
                "3D"
            ],
            "Year": "2018",
            "Bibtex": "'@inproceedings{10.1145/3170427.3188551,\nauthor = {Sun, Zhida and Han, Feng and Ma, Xiaojuan},\ntitle = {Exploring the Effects of Scale in Augmented Reality-Empowered Visual Analytics},\nyear = {2018},\nisbn = {9781450356213},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3170427.3188551},\ndoi = {10.1145/3170427.3188551},\nabstract = {Emerging Augmented Reality (AR) technologies can enable situated interactive visual analytics beyond the screen. However, the presentation and interaction design of data visualization integrated into the physical environment may vary in different scales. Understanding how users manage their spatial relationships with AR visualization under different representational scales is crucial for designing user-friendly AR-empowered visual analytic systems. To this end, we present a study with 16 participants, inviting them to solve two logical reasoning puzzles by interacting with the associated node-link graphs in AR in room- and table-scales respectively. Through observation, interviews, and video analysis, we identify three types of spatial arrangements, which are, positioning the visualization in the figural, vista, or panoramic space of a user. We further explore how scales and visualization design affect users' spatial preferences and exploratory behaviors, and summarize our findings among the three types of spatial arrangements.},\nbooktitle = {Extended Abstracts of the 2018 CHI Conference on Human Factors in Computing Systems},\npages = {1\u20136},\nnumpages = {6},\nkeywords = {user behavior, augmented reality, visual analytics, scale},\nlocation = {Montreal QC, Canada},\nseries = {CHI EA '18}\n}",
            "DOI": "10.1145/3170427.3188551",
            "Authors": [
                "Sun",
                "Zhida and Han",
                "Feng and Ma",
                "Xiaojuan"
            ]
        },
        {
            "Name": "FlyAR: Augmented Reality Supported Micro Aerial Vehicle Navigation",
            "Opportunity": [
                "Situated Analytics"
            ],
            "Contribution Type": [
                "System"
            ],
            "Data Domain": [
                "PNT"
            ],
            "Dataset Types": [
                "Spatial"
            ],
            "Dataset Generation": [
                "Dynamic",
                "Live-computation",
                "Interactive"
            ],
            "Presentation": [
                "AR",
                "Tablet",
                "Handheld"
            ],
            "Device": [
                "Motion J3400"
            ],
            "Input": [
                "Mobile Device"
            ],
            "Environment": [
                "Outdoors"
            ],
            "Space": [
                "Situated"
            ],
            "Embodiment": [
                "Exocentric"
            ],
            "Collaboration ": [
                "None"
            ],
            "Interaction": [
                "Touch",
                "Move"
            ],
            "Visualization": [
                "3D Trajectory"
            ],
            "Abstract/Natural": [
                "Natural Spatial Mapping"
            ],
            "Manipulate": [
                "Navigate",
                "Select",
                "Arrange"
            ],
            "Position": [
                "Fixed (Point)"
            ],
            "Scale": [
                "Large"
            ],
            "2D or 3D": [
                "3D"
            ],
            "Year": "2014",
            "Bibtex": "@ARTICLE{6777462,\n  author={Zollmann, Stefanie and Hoppe, Christof and Langlotz, Tobias and Reitmayr, Gerhard},\n  journal={IEEE Transactions on Visualization and Computer Graphics}, \n  title={FlyAR: Augmented Reality Supported Micro Aerial Vehicle Navigation}, \n  year={2014},\n  volume={20},\n  number={4},\n  pages={560-568},\n  doi={10.1109/TVCG.2014.24}}",
            "DOI": "10.1109/TVCG.2014.24",
            "Authors": [
                "S. Zollmann; C. Hoppe; T. Langlotz; G. Reitmayr"
            ]
        },
        {
            "Name": "GeoGate: Correlating Geo-Temporal Datasets Using an Augmented Reality Space-Time Cube and Tangible Interactions",
            "Opportunity": [
                "Embodied Data Exploration"
            ],
            "Contribution Type": [
                "System"
            ],
            "Data Domain": [
                "PNT"
            ],
            "Dataset Types": [
                "Spatial"
            ],
            "Dataset Generation": [
                "Static"
            ],
            "Presentation": [
                "AR",
                "LID",
                "HMD",
                "Tabletop"
            ],
            "Device": [
                "Hololens"
            ],
            "Input": [
                "Custom Controller",
                "Mouse/Keyboard"
            ],
            "Environment": [
                "Room Scale",
                "Table",
                "Object"
            ],
            "Space": [
                "Situated"
            ],
            "Embodiment": [
                "Exocentric"
            ],
            "Collaboration ": [
                "Synchronous Co-located"
            ],
            "Interaction": [
                "Move",
                "Raycast"
            ],
            "Visualization": [
                "2D Trajectory",
                "Space-Time Cube",
                "2D Map"
            ],
            "Abstract/Natural": [
                "Natural Spatial Mapping"
            ],
            "Manipulate": [
                "Zoom",
                "Rotate",
                "Filter",
                "Select",
                "Pan"
            ],
            "Position": [
                "Fixed (Point)"
            ],
            "Scale": [
                "Medium",
                "Small"
            ],
            "2D or 3D": [
                "2D"
            ],
            "Year": "2019",
            "Bibtex": "'@INPROCEEDINGS{8797812,\n  author={Ssin, Seung Youb and Walsh, James A. and Smith, Ross T. and Cunningham, Andrew and Thomas, Bruce H.},\n  booktitle={2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, \n  title={GeoGate: Correlating Geo-Temporal Datasets Using an Augmented Reality Space-Time Cube and Tangible Interactions}, \n  year={2019},\n  volume={},\n  number={},\n  pages={210-219},\n  doi={10.1109/VR.2019.8797812}}",
            "DOI": "10.1109/VR.2019.8797812",
            "Authors": [
                "S. Y. Ssin; J. A. Walsh; R. T. Smith; A. Cunningham; B. H. Thomas"
            ]
        },
        {
            "Name": "HybridAxes: An Immersive Analytics Tool With Interoperability Between 2D and Immersive Reality Modes",
            "Opportunity": [
                "Spatial Immersion"
            ],
            "Contribution Type": [
                "System"
            ],
            "Data Domain": [
                "General"
            ],
            "Dataset Types": [
                "Tabular"
            ],
            "Dataset Generation": [
                "Interactive",
                "Live-computation",
                "Dynamic"
            ],
            "Presentation": [
                "HMD",
                "AR"
            ],
            "Device": [
                "HTC vive"
            ],
            "Input": [
                "Commercial Controller",
                "Mouse/Keyboard"
            ],
            "Environment": [
                "Seated",
                "Object"
            ],
            "Space": [
                "Finite"
            ],
            "Embodiment": [
                "Exocentric"
            ],
            "Collaboration ": [
                "None"
            ],
            "Interaction": [
                "Grab",
                "Type",
                "Click"
            ],
            "Visualization": [
                "2D Scatter Plot",
                "2D Bar Chart",
                "2D Line Chart"
            ],
            "Abstract/Natural": [
                "Abstract"
            ],
            "Manipulate": [
                "Translate",
                "Arrange"
            ],
            "Position": [
                "Fixed (Object)"
            ],
            "Scale": [
                "Small"
            ],
            "2D or 3D": [
                "2D"
            ],
            "Year": "2022",
            "Bibtex": "'@INPROCEEDINGS{9974239,\nauthor={Seraji, Mohammad Rajabi and Stuerzlinger, Wolfgang},\nbooktitle={2022 IEEE International Symposium on Mixed and Augmented Reality Adjunct (ISMAR-Adjunct)},\ntitle={HybridAxes: An Immersive Analytics Tool With Interoperability Between 2D and Immersive Reality Modes},\nyear={2022},\nvolume={},\nnumber={},\npages={155-160},\ndoi={10.1109/ISMAR-Adjunct57072.2022.00036}}",
            "DOI": "10.1109/ISMAR-Adjunct57072.2022.00036",
            "Authors": [
                "Seraji",
                "Mohammad Rajabi and Stuerzlinger",
                "Wolfgang"
            ]
        },
        {
            "Name": "ImAxes: Immersive Axes as Embodied Affordances for Interactive Multivariate Data Visualisation",
            "Opportunity": [
                "Embodied Data Exploration"
            ],
            "Contribution Type": [
                "System"
            ],
            "Data Domain": [
                "General"
            ],
            "Dataset Types": [
                "Tabular"
            ],
            "Dataset Generation": [
                "Static",
                "Pre-computation"
            ],
            "Presentation": [
                "VR",
                "HMD"
            ],
            "Device": [
                "HTC vive"
            ],
            "Input": [
                "Commercial Controller"
            ],
            "Environment": [
                "Room Scale"
            ],
            "Space": [
                "Infinite"
            ],
            "Embodiment": [
                "Exocentric"
            ],
            "Collaboration ": [
                "None"
            ],
            "Interaction": [
                "Grab",
                "Slide",
                "Move"
            ],
            "Visualization": [
                "2D Bar Chart",
                "Parallel Coord",
                "2D Scatter Plot",
                "3D Scatter Plot",
                "3D Parallel Coord"
            ],
            "Abstract/Natural": [
                "Abstract"
            ],
            "Manipulate": [
                "Filter",
                "Scale",
                "Change",
                "Arrange",
                "Rotate"
            ],
            "Position": [
                "Unfixed"
            ],
            "Scale": [
                "Medium",
                "Small"
            ],
            "2D or 3D": [
                "2D",
                "3D"
            ],
            "Year": "2017",
            "Bibtex": "@inproceedings{10.1145/3126594.3126613,\nauthor = {Cordeil, Maxime and Cunningham, Andrew and Dwyer, Tim and Thomas, Bruce H. and Marriott, Kim},\ntitle = {ImAxes: Immersive Axes as Embodied Affordances for Interactive Multivariate Data Visualisation},\nyear = {2017},\nisbn = {9781450349819},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3126594.3126613},\ndoi = {10.1145/3126594.3126613},\nabstract = {We introduce ImAxes immersive system for exploring multivariate data using fluid, modeless interaction. The basic interface element is an embodied data axis. The user can manipulate these axes like physical objects in the immersive environment and combine them into sophisticated visualisations. The type of visualisation that appears depends on the proximity and relative orientation of the axes with respect to one another, which we describe with a formal grammar. This straight-forward composability leads to a number of emergent visualisations and interactions, which we review, and then demonstrate with a detailed multivariate data analysis use case.},\nbooktitle = {Proceedings of the 30th Annual ACM Symposium on User Interface Software and Technology},\npages = {71\u201383},\nnumpages = {13},\nkeywords = {immersive analytics, multidimensional data visualization, information visualization, immersion, virtual reality, immersive visualization},\nlocation = {Qu\\'{e}bec City, QC, Canada},\nseries = {UIST '17}\n}",
            "DOI": "10.1145/3126594.3126613",
            "Authors": [
                "Cordeil",
                "Maxime and Cunningham",
                "Andrew and Dwyer",
                "Tim and Thomas",
                "Bruce H. and Marriott",
                "Kim"
            ]
        },
        {
            "Name": "Immersive Collaborative Analysis of Network Connectivity: CAVE-style or Head-Mounted Display?",
            "Opportunity": [
                "Collaboration"
            ],
            "Contribution Type": [
                "Evaluation"
            ],
            "Data Domain": [
                "General"
            ],
            "Dataset Types": [
                "Network"
            ],
            "Dataset Generation": [
                "Static",
                "Pre-computation"
            ],
            "Presentation": [
                "HMD",
                "VR",
                "LID"
            ],
            "Device": [
                "CAVE 2"
            ],
            "Input": [
                "Commercial Controller",
                "Gesture"
            ],
            "Environment": [
                "Room Scale",
                "Wall",
                "Seated"
            ],
            "Space": [
                "Finite"
            ],
            "Embodiment": [
                "Egocentric"
            ],
            "Collaboration ": [
                "Synchronous Co-located"
            ],
            "Interaction": [
                "Touch",
                "Raycast",
                "Move"
            ],
            "Visualization": [
                "3D Node Link"
            ],
            "Abstract/Natural": [
                "Abstract"
            ],
            "Manipulate": [
                "Navigate",
                "Select"
            ],
            "Position": [
                "Fixed (Point)"
            ],
            "Scale": [
                "Large",
                "Medium"
            ],
            "2D or 3D": [
                "3D"
            ],
            "Year": "2017",
            "Bibtex": "@ARTICLE{7539620,\n  author={Cordeil, Maxime and Dwyer, Tim and Klein, Karsten and Laha, Bireswar and Marriott, Kim and Thomas, Bruce H.},\n  journal={IEEE Transactions on Visualization and Computer Graphics}, \n  title={Immersive Collaborative Analysis of Network Connectivity: CAVE-style or Head-Mounted Display?}, \n  year={2017},\n  volume={23},\n  number={1},\n  pages={441-450},\n  doi={10.1109/TVCG.2016.2599107}}",
            "DOI": "10.1109/TVCG.2016.2599107",
            "Authors": [
                "M. Cordeil; T. Dwyer; K. Klein; B. Laha; K. Marriott; B. H. Thomas"
            ]
        },
        {
            "Name": "Immersive ParaView: An Immersive Scientific Workflow for the Advancement of Measurement Science",
            "Opportunity": [
                "Spatial Immersion"
            ],
            "Contribution Type": [
                "System"
            ],
            "Data Domain": [
                "General"
            ],
            "Dataset Types": [
                "Tabular",
                "Fields"
            ],
            "Dataset Generation": [
                "Static",
                "Pre-computation"
            ],
            "Presentation": [
                "LID"
            ],
            "Device": [
                "CAVE"
            ],
            "Input": [
                "Commercial Controller"
            ],
            "Environment": [
                "Room Scale"
            ],
            "Space": [
                "Infinite"
            ],
            "Embodiment": [
                "Egocentric"
            ],
            "Collaboration ": [
                "Synchronous Co-located"
            ],
            "Interaction": [
                "Raycast",
                "Grab"
            ],
            "Visualization": [
                "3D Flow Network",
                "Anatomical"
            ],
            "Abstract/Natural": [
                "Abstract",
                "Natural Spatial Mapping"
            ],
            "Manipulate": [
                "Select",
                "Scale",
                "Translate"
            ],
            "Position": [
                "Unfixed"
            ],
            "Scale": [
                "Large"
            ],
            "2D or 3D": [
                "3D"
            ],
            "Year": "2022",
            "Bibtex": "'@INPROCEEDINGS{9974489,\nauthor={Su, Simon and Lopez-Coto, Israel and Sherman, William R. and Sayrafian, Kamran and Terrill, Judith},\nbooktitle={2022 IEEE International Symposium on Mixed and Augmented Reality Adjunct (ISMAR-Adjunct)},\ntitle={Immersive ParaView: An Immersive Scientific Workflow for the Advancement of Measurement Science},\nyear={2022},\nvolume={},\nnumber={},\npages={139-145},\ndoi={10.1109/ISMAR-Adjunct57072.2022.00034}}",
            "DOI": "10.1109/ISMAR-Adjunct57072.2022.00034",
            "Authors": [
                "Su",
                "Simon and Lopez-Coto",
                "Israel and Sherman",
                "William R. and Sayrafian",
                "Kamran and Terrill",
                "Judith"
            ]
        },
        {
            "Name": "Immersive WYSIWYG virtual meteorological sandbox",
            "Opportunity": [
                "Embodied Data Exploration"
            ],
            "Contribution Type": [
                "System"
            ],
            "Data Domain": [
                "Meteorological",
                "Physical Sciences"
            ],
            "Dataset Types": [
                "Tabular",
                "Spatial"
            ],
            "Dataset Generation": [
                "Static",
                "Pre-computation"
            ],
            "Presentation": [
                "HMD",
                "VR"
            ],
            "Device": [
                "Oculus Rift S"
            ],
            "Input": [
                "Gesture"
            ],
            "Environment": [
                "Room Scale"
            ],
            "Space": [
                "Finite"
            ],
            "Embodiment": [
                "Exocentric"
            ],
            "Collaboration ": [
                "None"
            ],
            "Interaction": [
                "Grab",
                "Pinch",
                "Touch",
                "Rotate"
            ],
            "Visualization": [
                "3D Scatter Plot",
                "3D Bar Chart",
                "Time-Line",
                "3D Map"
            ],
            "Abstract/Natural": [
                "Natural Spatial Mapping",
                "Abstract"
            ],
            "Manipulate": [
                "Translate",
                "Rotate",
                "Scale",
                "Select",
                "Zoom"
            ],
            "Position": [
                "Unfixed"
            ],
            "Scale": [
                "Medium",
                "Large"
            ],
            "2D or 3D": [
                "3D"
            ],
            "Year": "2022",
            "Bibtex": "'@INPROCEEDINGS{9974250,\n  author={Hu, Hao and Wang, Song and Chen, Yonghui},\n  booktitle={2022 IEEE International Symposium on Mixed and Augmented Reality Adjunct (ISMAR-Adjunct)}, \n  title={Immersive WYSIWYG virtual meteorological sandbox}, \n  year={2022},\n  volume={},\n  number={},\n  pages={131-138},\n  doi={10.1109/ISMAR-Adjunct57072.2022.00033}}",
            "DOI": "10.1109/ISMAR-Adjunct57072.2022.00033",
            "Authors": [
                "Hao Hu; Song Wang; Yonghui Chen"
            ]
        },
        {
            "Name": "ImNDT: Immersive Workspace for the Analysis of Multidimensional Material Data From Non-Destructive Testing",
            "Opportunity": [
                "Spatial Immersion"
            ],
            "Contribution Type": [
                "System"
            ],
            "Data Domain": [
                "Physical Sciences",
                "Material Science"
            ],
            "Dataset Types": [
                "Volumetric data",
                "Tabular"
            ],
            "Dataset Generation": [
                "Static"
            ],
            "Presentation": [
                "VR",
                "HMD"
            ],
            "Device": [
                "HTC Vive pro"
            ],
            "Input": [
                "Commercial Controller"
            ],
            "Environment": [
                "Room Scale"
            ],
            "Space": [
                "Infinite"
            ],
            "Embodiment": [
                "Egocentric"
            ],
            "Collaboration ": [
                "None"
            ],
            "Interaction": [
                "Click",
                "Touch",
                "Move"
            ],
            "Visualization": [
                "2D Bar Chart",
                "3D Trajectory",
                "Volume Visualization",
                "3D Matrix/Heatmap"
            ],
            "Abstract/Natural": [
                "Abstract"
            ],
            "Manipulate": [
                "Select",
                "Scale",
                "Arrange",
                "Navigate"
            ],
            "Position": [
                "Unfixed",
                "Fixed (Body)"
            ],
            "Scale": [
                "Small",
                "Large"
            ],
            "2D or 3D": [
                "2D",
                "3D"
            ],
            "Year": "2021",
            "Bibtex": "'@inproceedings{10.1145/3489849.3489851,\nauthor = {Gall, Alexander and Gr\\\"{o}ller, Eduard and Heinzl, Christoph},\ntitle = {ImNDT: Immersive Workspace for the Analysis of Multidimensional Material Data From Non-Destructive Testing},\nyear = {2021},\nisbn = {9781450390927},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3489849.3489851},\ndoi = {10.1145/3489849.3489851},\nabstract = {An analysis of large multidimensional volumetric data as generated by non-destructive testing (NDT) techniques, e.g., X-ray computed tomography (XCT), can hardly be evaluated using standard 2D visualization techniques on desktop monitors. The analysis of fiber-reinforced polymers (FRPs) is currently a time-consuming and cognitively demanding task, as FRPs have a complex spatial structure, consisting of several hundred thousand fibers, each having more than twenty different extracted features. This paper presents ImNDT, a novel visualization system, which offers material experts an immersive exploration of multidimensional secondary data of FRPs. Our system is based on a virtual reality (VR) head-mounted device (HMD) to enable fluid and natural explorations through embodied navigation, the avoidance of menus, and manual mode switching. We developed immersive visualization and interaction methods tailored to the characterization of FRPs, such as a Model in Miniature, a similarity network, and a histo-book. An evaluation of our techniques with domain experts showed advantages in discovering structural patterns and similarities. Especially novices can strongly benefit from our intuitive representation and spatial rendering.},\nbooktitle = {Proceedings of the 27th ACM Symposium on Virtual Reality Software and Technology},\narticleno = {9},\nnumpages = {11},\nkeywords = {interaction techniques, Multidimensional data visualization, immersive analytics, virtual reality, fibre-reinforced polymers},\nlocation = {Osaka, Japan},\nseries = {VRST '21}\n}",
            "DOI": "10.1145/3489849.3489851",
            "Authors": [
                "Gall",
                "Alexander and Gr\\\"{o}ller",
                "Eduard and Heinzl",
                "Christoph"
            ]
        },
        {
            "Name": "Improving Information Sharing and Collaborative Analysis for Remote GeoSpatial Visualization Using Mixed Reality",
            "Opportunity": [
                "Collaboration"
            ],
            "Contribution Type": [
                "System"
            ],
            "Data Domain": [
                "General"
            ],
            "Dataset Types": [
                "Spatial",
                "Tabular"
            ],
            "Dataset Generation": [
                "Static",
                "Pre-computation"
            ],
            "Presentation": [
                "AR",
                "HMD"
            ],
            "Device": [
                "Hololens"
            ],
            "Input": [
                "Voice",
                "Gesture",
                "Gaze"
            ],
            "Environment": [
                "Room Scale"
            ],
            "Space": [
                "Finite"
            ],
            "Embodiment": [
                "Exocentric"
            ],
            "Collaboration ": [
                "Synchronous Distributed"
            ],
            "Interaction": [
                "Pinch",
                "Speak",
                "Gesture",
                "Gaze"
            ],
            "Visualization": [
                "2D Map",
                "3D Map"
            ],
            "Abstract/Natural": [
                "Natural Spatial Mapping"
            ],
            "Manipulate": [
                "Change",
                "Arrange",
                "Filter",
                "Select"
            ],
            "Position": [
                "Unfixed",
                "Fixed (Point)"
            ],
            "Scale": [
                "Medium"
            ],
            "2D or 3D": [
                "2D",
                "3D"
            ],
            "Year": "2019",
            "Bibtex": "'@INPROCEEDINGS{8943760,\n  author={Mahmood, Tahir and Fulmer, Willis and Mungoli, Neelesh and Huang, Jian and Lu, Aidong},\n  booktitle={2019 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, \n  title={Improving Information Sharing and Collaborative Analysis for Remote GeoSpatial Visualization Using Mixed Reality}, \n  year={2019},\n  volume={},\n  number={},\n  pages={236-247},\n  doi={10.1109/ISMAR.2019.00021}}",
            "DOI": "10.1109/ISMAR.2019.00021",
            "Authors": [
                "T. Mahmood; W. Fulmer; N. Mungoli; J. Huang; A. Lu"
            ]
        },
        {
            "Name": "Infocarve: A Framework for Volume Visualization on Commodity Augmented Reality Displays",
            "Opportunity": [
                "Situated Analytics"
            ],
            "Contribution Type": [
                "Technique"
            ],
            "Data Domain": [
                "Biology",
                "Life sciences"
            ],
            "Dataset Types": [
                "Volumetric data"
            ],
            "Dataset Generation": [
                "Static",
                "Pre-computation"
            ],
            "Presentation": [
                "HMD",
                "AR"
            ],
            "Device": [
                "Epson Moverio",
                "Metavision Meta 1",
                "Intel Realsense f200",
                "Google project tango tablet"
            ],
            "Input": [
                "None"
            ],
            "Environment": [
                "Room Scale",
                "Object"
            ],
            "Space": [
                "Situated"
            ],
            "Embodiment": [
                "Exocentric"
            ],
            "Collaboration ": [
                "None"
            ],
            "Interaction": [
                "None"
            ],
            "Visualization": [
                "Volume Visualization",
                "Anatomical"
            ],
            "Abstract/Natural": [
                "Natural Spatial Mapping"
            ],
            "Manipulate": [
                "None"
            ],
            "Position": [
                "Fixed (Object)"
            ],
            "Scale": [
                "Small"
            ],
            "2D or 3D": [
                "3D"
            ],
            "Year": "2016",
            "Bibtex": "'@INPROCEEDINGS{7938241,\n  author={Ran, Lingqiang and Dingliana, John},\n  booktitle={2016 International Conference on Virtual Reality and Visualization (ICVRV)}, \n  title={Infocarve: A Framework for Volume Visualization on Commodity Augmented Reality Displays}, \n  year={2016},\n  volume={},\n  number={},\n  pages={473-479},\n  doi={10.1109/ICVRV.2016.86}}",
            "DOI": "10.1109/ICVRV.2016.86",
            "Authors": [
                "L. Ran; J. Dingliana"
            ]
        },
        {
            "Name": "Information Olfactation: Harnessing Scent to Convey Data",
            "Opportunity": [
                "Multi-Sensory Presentation"
            ],
            "Contribution Type": [
                "Technique"
            ],
            "Data Domain": [
                "Economy",
                "Environmental"
            ],
            "Dataset Types": [
                "Network"
            ],
            "Dataset Generation": [
                "Static"
            ],
            "Presentation": [
                "HMD",
                "VR",
                "Scent"
            ],
            "Device": [
                "HTC vive",
                "Custom"
            ],
            "Input": [
                "Commercial Controller"
            ],
            "Environment": [
                "Room Scale"
            ],
            "Space": [
                "Finite"
            ],
            "Embodiment": [
                "Exocentric"
            ],
            "Collaboration ": [
                "None"
            ],
            "Interaction": [
                "Grab",
                "Click"
            ],
            "Visualization": [
                "3D Node Link",
                "2D Node Link",
                "2D Line Chart"
            ],
            "Abstract/Natural": [
                "Abstract"
            ],
            "Manipulate": [
                "Select",
                "Navigate"
            ],
            "Position": [
                "Unfixed"
            ],
            "Scale": [
                "Large"
            ],
            "2D or 3D": [
                "3D",
                "2D"
            ],
            "Year": "2019",
            "Bibtex": "@ARTICLE{8444077,\n  author={Patnaik, Biswaksen and Batch, Andrea and Elmqvist, Niklas},\n  journal={IEEE Transactions on Visualization and Computer Graphics}, \n  title={Information Olfactation: Harnessing Scent to Convey Data}, \n  year={2019},\n  volume={25},\n  number={1},\n  pages={726-736},\n  doi={10.1109/TVCG.2018.2865237}}",
            "DOI": "10.1109/TVCG.2018.2865237",
            "Authors": [
                "B. Patnaik; A. Batch; N. Elmqvist"
            ]
        },
        {
            "Name": "Interactive Visualization of Deep Learning Models in an Immersive Environment",
            "Opportunity": [
                "Spatial Immersion"
            ],
            "Contribution Type": [
                "System"
            ],
            "Data Domain": [
                "Deep learning"
            ],
            "Dataset Types": [
                "Tabular"
            ],
            "Dataset Generation": [
                "Static",
                "Pre-computation"
            ],
            "Presentation": [
                "VR",
                "HMD"
            ],
            "Device": [
                "Oculus Quest 2"
            ],
            "Input": [
                "Commercial Controller"
            ],
            "Environment": [
                "Room Scale"
            ],
            "Space": [
                "Infinite"
            ],
            "Embodiment": [
                "Egocentric"
            ],
            "Collaboration ": [
                "None"
            ],
            "Interaction": [
                "Gesture",
                "Touch"
            ],
            "Visualization": [
                "2D Line Chart",
                "Volume Visualization",
                "2D Matrix/Heatmap"
            ],
            "Abstract/Natural": [
                "Abstract"
            ],
            "Manipulate": [
                "Arrange",
                "Change",
                "Navigate"
            ],
            "Position": [
                "Fixed (Point)"
            ],
            "Scale": [
                "Medium"
            ],
            "2D or 3D": [
                "2D",
                "3D"
            ],
            "Year": "2021",
            "Bibtex": "'@inproceedings{10.1145/3489849.3489956,\nauthor = {Nagasaka, Hikaru and Izuhara, Motoya},\ntitle = {Interactive Visualization of Deep Learning Models in an Immersive Environment},\nyear = {2021},\nisbn = {9781450390927},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3489849.3489956},\ndoi = {10.1145/3489849.3489956},\nabstract = {The development of deep learning (DL) models has been prevalent among software engineers. However, it is difficult for non-experts to analyze and understand their behavior. Hence, we propose an interactive visualization system of DL models in an immersive environment. Because an immersive environment offers unlimited displays and visualization of high-dimensional data, it enables a comprehensive analysis on data propagations through the layers, and compares the multiple performance metrics. In this research, we implemented a prototype system, demonstrated it to machine learning engineers, and discussed the future benefits of visualizing DL models in an immersive environment. Accordingly, our concept received positive feedback; however, we inferred that most of the engineers consider the visualization technology as a unique introduction to the immersive environment.},\nbooktitle = {Proceedings of the 27th ACM Symposium on Virtual Reality Software and Technology},\narticleno = {82},\nnumpages = {3},\nkeywords = {immersive analytics, deep learning},\nlocation = {Osaka, Japan},\nseries = {VRST '21}\n}",
            "DOI": "10.1145/3489849.3489956",
            "Authors": [
                "Nagasaka",
                "Hikaru and Izuhara",
                "Motoya"
            ]
        },
        {
            "Name": "Investigating the Impact of Real-World Environments on the Perception of 2D Visualizations in Augmented Reality",
            "Opportunity": [
                "Situated Analytics"
            ],
            "Contribution Type": [
                "Evaluation"
            ],
            "Data Domain": [
                "Manufacturing",
                "General"
            ],
            "Dataset Types": [
                "Tabular"
            ],
            "Dataset Generation": [
                "Dynamic",
                "Live-computation"
            ],
            "Presentation": [
                "HMD",
                "AR"
            ],
            "Device": [
                "Hololens"
            ],
            "Input": [
                "Commercial Controller"
            ],
            "Environment": [
                "Room Scale",
                "Object"
            ],
            "Space": [
                "Situated"
            ],
            "Embodiment": [
                "Egocentric"
            ],
            "Collaboration ": [
                "None"
            ],
            "Interaction": [
                "Raycast",
                "Click"
            ],
            "Visualization": [
                "2D Line Chart"
            ],
            "Abstract/Natural": [
                "Abstract"
            ],
            "Manipulate": [
                "Select",
                "Arrange"
            ],
            "Position": [
                "Fixed (Object)"
            ],
            "Scale": [
                "Medium"
            ],
            "2D or 3D": [
                "2D"
            ],
            "Year": "2021",
            "Bibtex": "'@inproceedings{10.1145/3411764.3445330,\nauthor = {Satkowski, Marc and Dachselt, Raimund},\ntitle = {Investigating the Impact of Real-World Environments on the Perception of 2D Visualizations in Augmented Reality},\nyear = {2021},\nisbn = {9781450380966},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3411764.3445330},\ndoi = {10.1145/3411764.3445330},\nabstract = {In this work we report on two comprehensive user studies investigating the perception of Augmented Reality (AR) visualizations influenced by real-world backgrounds. Since AR is an emerging technology, it is important to also consider productive use cases, which is why we chose an exemplary and challenging industry 4.0 environment. Our basic perceptual research focuses on both the visual complexity of backgrounds as well as the influence of a secondary task. In contrast to our expectation, data of our 34 study participants indicate that the background has far less influence on the perception of AR visualizations. Moreover, we observed a mismatch between measured and subjectively reported performance. We discuss the importance of the background and recommendations for visual real-world augmentations. Overall, our results suggest that AR can be used in many visually challenging environments without losing the ability to productively work with the visualizations shown.},\nbooktitle = {Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems},\narticleno = {522},\nnumpages = {15},\nkeywords = {Augmented Reality, User Study, Industrial Scenario, AR Visualization, Immersive Analytics, In-Situ Visualization, Visual Perception, Industry 4.0},\nlocation = {Yokohama, Japan},\nseries = {CHI '21}\n}",
            "DOI": "10.1145/3411764.3445330",
            "Authors": [
                "Satkowski",
                "Marc and Dachselt",
                "Raimund"
            ]
        },
        {
            "Name": "Location-Based Augmented Reality In-situ Visualization Applied for Agricultural Fieldwork Navigation",
            "Opportunity": [
                "Situated Analytics"
            ],
            "Contribution Type": [
                "System"
            ],
            "Data Domain": [
                "Agriculture"
            ],
            "Dataset Types": [
                "Spatial",
                "Tabular"
            ],
            "Dataset Generation": [
                "Static",
                "Pre-computation"
            ],
            "Presentation": [
                "AR",
                "Tablet",
                "HMD",
                "VR"
            ],
            "Device": [
                "Samsung Galaxy Tab Se5",
                "HTC vive Focus"
            ],
            "Input": [
                "Voice",
                "Gaze"
            ],
            "Environment": [
                "Outdoors"
            ],
            "Space": [
                "Situated"
            ],
            "Embodiment": [
                "Exocentric"
            ],
            "Collaboration ": [
                "None"
            ],
            "Interaction": [
                "Gaze",
                "Speak"
            ],
            "Visualization": [
                "2D Bar Chart",
                "Area Chart",
                "2D Trajectory"
            ],
            "Abstract/Natural": [
                "Natural Spatial Mapping",
                "Abstract"
            ],
            "Manipulate": [
                "Select",
                "Arrange"
            ],
            "Position": [
                "Fixed (Point)",
                "Fixed (Body)"
            ],
            "Scale": [
                "Large"
            ],
            "2D or 3D": [
                "2D"
            ],
            "Year": "2019",
            "Bibtex": "'@INPROCEEDINGS{8951987,\n  author={Zheng, Mengya and Campbell, Abraham G.},\n  booktitle={2019 IEEE International Symposium on Mixed and Augmented Reality Adjunct (ISMAR-Adjunct)}, \n  title={Location-Based Augmented Reality In-situ Visualization Applied for Agricultural Fieldwork Navigation}, \n  year={2019},\n  volume={},\n  number={},\n  pages={93-97},\n  doi={10.1109/ISMAR-Adjunct.2019.00039}}",
            "DOI": "10.1109/ISMAR-Adjunct.2019.00039",
            "Authors": [
                "M. Zheng; A. G. Campbell"
            ]
        },
        {
            "Name": "Look Inside: Understanding Thermal Flux Through Augmented Reality",
            "Opportunity": [
                "Situated Analytics"
            ],
            "Contribution Type": [
                "System"
            ],
            "Data Domain": [
                "Physical Sciences"
            ],
            "Dataset Types": [
                "Tabular"
            ],
            "Dataset Generation": [
                "Live-computation",
                "Dynamic"
            ],
            "Presentation": [
                "HMD",
                "AR"
            ],
            "Device": [
                "Hololens"
            ],
            "Input": [
                "Gesture"
            ],
            "Environment": [
                "Seated"
            ],
            "Space": [
                "Situated"
            ],
            "Embodiment": [
                "Egocentric"
            ],
            "Collaboration ": [
                "None"
            ],
            "Interaction": [
                "Touch",
                "Gesture"
            ],
            "Visualization": [
                "2D Line Chart",
                "2D Matrix/Heatmap"
            ],
            "Abstract/Natural": [
                "Abstract",
                "Natural Spatial Mapping"
            ],
            "Manipulate": [
                "Arrange"
            ],
            "Position": [
                "Fixed (Object)"
            ],
            "Scale": [
                "Small"
            ],
            "2D or 3D": [
                "2D"
            ],
            "Year": "2018",
            "Bibtex": "'@INPROCEEDINGS{8699216,\n  author={Knierim, Pascal and Kiss, Francisco and Schmidt, Albrecht},\n  booktitle={2018 IEEE International Symposium on Mixed and Augmented Reality Adjunct (ISMAR-Adjunct)}, \n  title={Look Inside: Understanding Thermal Flux Through Augmented Reality}, \n  year={2018},\n  volume={},\n  number={},\n  pages={170-171},\n  doi={10.1109/ISMAR-Adjunct.2018.00059}}",
            "DOI": "10.1109/ISMAR-Adjunct.2018.00059",
            "Authors": [
                "P. Knierim; F. Kiss; A. Schmidt"
            ]
        },
        {
            "Name": "MARVIS: Combining Mobile Devices and Augmented Reality for Visual Data Analysis",
            "Opportunity": [
                "Situated Analytics"
            ],
            "Contribution Type": [
                "System"
            ],
            "Data Domain": [
                "General"
            ],
            "Dataset Types": [
                "Tabular"
            ],
            "Dataset Generation": [
                "Static"
            ],
            "Presentation": [
                "AR",
                "HMD",
                "Tablet",
                "Tabletop"
            ],
            "Device": [
                "Hololens"
            ],
            "Input": [
                "Mobile Device"
            ],
            "Environment": [
                "Seated",
                "Object"
            ],
            "Space": [
                "Situated"
            ],
            "Embodiment": [
                "Egocentric"
            ],
            "Collaboration ": [
                "Synchronous Co-located"
            ],
            "Interaction": [
                "Touch",
                "Move",
                "Pinch",
                "Rotate",
                "Brushing"
            ],
            "Visualization": [
                "2D Bar Chart",
                "3D Trajectory",
                "3D Bar Chart",
                "Parallel Coord",
                "2D Scatter Plot",
                "2D Violin Plots",
                "Glyph/Realism",
                "2D Matrix/Heatmap",
                "2D Map"
            ],
            "Abstract/Natural": [
                "Abstract",
                "Natural Spatial Mapping"
            ],
            "Manipulate": [
                "Select",
                "Change",
                "Translate",
                "Zoom",
                "Rotate",
                "Arrange"
            ],
            "Position": [
                "Fixed (Object)"
            ],
            "Scale": [
                "Medium"
            ],
            "2D or 3D": [
                "2D",
                "3D"
            ],
            "Year": "2021",
            "Bibtex": "'@inproceedings{10.1145/3411764.3445593,\nauthor = {Langner, Ricardo and Satkowski, Marc and B\\\"{u}schel, Wolfgang and Dachselt, Raimund},\ntitle = {MARVIS: Combining Mobile Devices and Augmented Reality for Visual Data Analysis},\nyear = {2021},\nisbn = {9781450380966},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3411764.3445593},\ndoi = {10.1145/3411764.3445593},\nabstract = {We present Marvis, a conceptual framework that combines mobile devices and head-mounted Augmented Reality (AR) for visual data analysis. We propose novel concepts and techniques addressing visualization-specific challenges. By showing additional 2D and 3D information around and above displays, we extend their limited screen space. AR views between displays as well as linking and brushing are also supported, making relationships between separated visualizations plausible. We introduce the design process and rationale for our techniques. To validate Marvis\u2019 concepts and show their versatility and widespread applicability, we describe six implemented example use cases. Finally, we discuss insights from expert hands-on reviews. As a result, we contribute to a better understanding of how the combination of one or more mobile devices with AR can benefit visual data analysis. By exploring this new type of visualization environment, we hope to provide a foundation and inspiration for future mobile data visualizations.},\nbooktitle = {Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems},\narticleno = {468},\nnumpages = {17},\nkeywords = {augmented displays, immersive analytics, mobile devices, head-mounted augmented reality, mobile data visualization, data analysis, cross-device interaction, data visualization},\nlocation = {Yokohama, Japan},\nseries = {CHI '21}\n}",
            "DOI": "10.1145/3411764.3445593",
            "Authors": [
                "Langner",
                "Ricardo and Satkowski",
                "Marc and B\\\"{u}schel",
                "Wolfgang and Dachselt",
                "Raimund"
            ]
        },
        {
            "Name": "MARVisT: Authoring Glyph-Based Visualization in Mobile Augmented Reality",
            "Opportunity": [
                "Engagement"
            ],
            "Contribution Type": [
                "Design Study"
            ],
            "Data Domain": [
                "General"
            ],
            "Dataset Types": [
                "Tabular"
            ],
            "Dataset Generation": [
                "Static",
                "Interactive",
                "Live-computation"
            ],
            "Presentation": [
                "AR",
                "Tablet",
                "Handheld"
            ],
            "Device": [
                "iPhone 8 Plus"
            ],
            "Input": [
                "Mobile Device"
            ],
            "Environment": [
                "Room Scale",
                "Object"
            ],
            "Space": [
                "Situated"
            ],
            "Embodiment": [
                "Exocentric"
            ],
            "Collaboration ": [
                "None"
            ],
            "Interaction": [
                "Touch",
                "Move"
            ],
            "Visualization": [
                "Glyph/Realism"
            ],
            "Abstract/Natural": [
                "Abstract",
                "Natural Spatial Mapping"
            ],
            "Manipulate": [
                "Change",
                "Translate",
                "Arrange",
                "Rotate",
                "Scale"
            ],
            "Position": [
                "Fixed (Object)"
            ],
            "Scale": [
                "Small",
                "Medium"
            ],
            "2D or 3D": [
                "3D"
            ],
            "Year": "2020",
            "Bibtex": "@ARTICLE{8611113,\n  author={Chen, Zhutian and Su, Yijia and Wang, Yifang and Wang, Qianwen and Qu, Huamin and Wu, Yingcai},\n  journal={IEEE Transactions on Visualization and Computer Graphics}, \n  title={MARVisT: Authoring Glyph-Based Visualization in Mobile Augmented Reality}, \n  year={2020},\n  volume={26},\n  number={8},\n  pages={2645-2658},\n  doi={10.1109/TVCG.2019.2892415}}",
            "DOI": "10.1109/TVCG.2019.2892415",
            "Authors": [
                "Z. Chen; Y. Su; Y. Wang; Q. Wang; H. Qu; Y. Wu"
            ]
        },
        {
            "Name": "Mixed-Reality Guidance for Brain Stimulation Treatment of Depression",
            "Opportunity": [
                "Situated Analytics"
            ],
            "Contribution Type": [
                "System"
            ],
            "Data Domain": [
                "Biology",
                "Life sciences"
            ],
            "Dataset Types": [
                "Fields"
            ],
            "Dataset Generation": [
                "Dynamic",
                "Live-computation"
            ],
            "Presentation": [
                "HMD",
                "AR"
            ],
            "Device": [
                "Hololens"
            ],
            "Input": [
                "Custom Controller"
            ],
            "Environment": [
                "Room Scale",
                "Object"
            ],
            "Space": [
                "Situated"
            ],
            "Embodiment": [
                "Exocentric"
            ],
            "Collaboration ": [
                "None"
            ],
            "Interaction": [
                "Move"
            ],
            "Visualization": [
                "Volume Visualization",
                "Anatomical"
            ],
            "Abstract/Natural": [
                "Natural Spatial Mapping"
            ],
            "Manipulate": [
                "Slice"
            ],
            "Position": [
                "Fixed (Object)"
            ],
            "Scale": [
                "Small"
            ],
            "2D or 3D": [
                "3D"
            ],
            "Year": "2018",
            "Bibtex": "'@INPROCEEDINGS{8699167,\n  author={Leuze, Christoph and Yang, Grant and Hargreaves, Brian and Daniel, Bruce and McNab, Jennifer A},\n  booktitle={2018 IEEE International Symposium on Mixed and Augmented Reality Adjunct (ISMAR-Adjunct)}, \n  title={Mixed-Reality Guidance for Brain Stimulation Treatment of Depression}, \n  year={2018},\n  volume={},\n  number={},\n  pages={377-380},\n  doi={10.1109/ISMAR-Adjunct.2018.00109}}",
            "DOI": "10.1109/ISMAR-Adjunct.2018.00109",
            "Authors": [
                "C. Leuze; G. Yang; B. Hargreaves; B. Daniel; J. A. McNab"
            ]
        },
        {
            "Name": "NIVR: Neuro imaging in virtual reality",
            "Opportunity": [
                "Spatial Immersion"
            ],
            "Contribution Type": [
                "System"
            ],
            "Data Domain": [
                "Biology",
                "Life sciences"
            ],
            "Dataset Types": [
                "Volumetric data"
            ],
            "Dataset Generation": [
                "Static",
                "Interactive",
                "Pre-computation"
            ],
            "Presentation": [
                "HMD",
                "VR"
            ],
            "Device": [
                "HTC vive"
            ],
            "Input": [
                "Commercial Controller"
            ],
            "Environment": [
                "Room Scale"
            ],
            "Space": [
                "Finite"
            ],
            "Embodiment": [
                "Egocentric"
            ],
            "Collaboration ": [
                "None"
            ],
            "Interaction": [
                "Grab",
                "Touch",
                "Raycast"
            ],
            "Visualization": [
                "Anatomical",
                "Volume Visualization"
            ],
            "Abstract/Natural": [
                "Natural Spatial Mapping"
            ],
            "Manipulate": [
                "Change",
                "Translate",
                "Slice",
                "Zoom"
            ],
            "Position": [
                "Fixed (Point)"
            ],
            "Scale": [
                "Medium"
            ],
            "2D or 3D": [
                "3D"
            ],
            "Year": "2017",
            "Bibtex": "'@INPROCEEDINGS{7892381,\n  author={Ard, Tyler and Krum, David M. and Phan, Thai and Duncan, Dominique and Essex, Ryan and Bolas, Mark and Toga, Arthur},\n  booktitle={2017 IEEE Virtual Reality (VR)}, \n  title={NIVR: Neuro imaging in virtual reality}, \n  year={2017},\n  volume={},\n  number={},\n  pages={465-466},\n  doi={10.1109/VR.2017.7892381}}",
            "DOI": "10.1109/VR.2017.7892381",
            "Authors": [
                "T. Ard; D. M. Krum; T. Phan; D. Duncan; R. Essex; M. Bolas; A. Toga"
            ]
        },
        {
            "Name": "OpticARe - Augmented Reality Mobile Patient Monitoring in Intensive Care Units",
            "Opportunity": [
                "Situated Analytics"
            ],
            "Contribution Type": [
                "Design Study"
            ],
            "Data Domain": [
                "Biology",
                "Health",
                "Life sciences"
            ],
            "Dataset Types": [
                "Tabular"
            ],
            "Dataset Generation": [
                "Dynamic"
            ],
            "Presentation": [
                "AR",
                "HMD"
            ],
            "Device": [
                "Hololens"
            ],
            "Input": [
                "None"
            ],
            "Environment": [
                "Room Scale",
                "Object"
            ],
            "Space": [
                "Finite"
            ],
            "Embodiment": [
                "Egocentric"
            ],
            "Collaboration ": [
                "None"
            ],
            "Interaction": [
                "None"
            ],
            "Visualization": [
                "2D Line Chart"
            ],
            "Abstract/Natural": [
                "Abstract"
            ],
            "Manipulate": [
                "None"
            ],
            "Position": [
                "Fixed (Object)"
            ],
            "Scale": [
                "Medium"
            ],
            "2D or 3D": [
                "2D"
            ],
            "Year": "2021",
            "Bibtex": "'@inproceedings{10.1145/3489849.3489852,\nauthor = {Kimmel, Simon and Cobus, Vanessa and Heuten, Wilko},\ntitle = {OpticARe - Augmented Reality Mobile Patient Monitoring in Intensive Care Units},\nyear = {2021},\nisbn = {9781450390927},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3489849.3489852},\ndoi = {10.1145/3489849.3489852},\nabstract = {German Intensive Care Units (ICUs) are in crisis, struggling with an increasing shortage of skilled workers, ultimately putting patients\u2019 safety at risk. To counteract this process, researchers are increasingly concerned with finding digital solutions which aim to support healthcare professionals by enhancing the efficiency of reoccurring critical caring tasks and thus, improve working conditions. In this regard, this paper evaluates the application of Augmented Reality (AR) for patient monitoring for critical care nursing. Grounded on an observational study, semi-structured interviews, as well as a quantitative analysis, mobile patient monitoring scenarios, present particularly during patient transport, were identified as an innovative context of use of AR in the field. Additionally, user requirements such as high wearability, hands-free operability, and clear data representation could be derived from the obtained study results. For validation of these and identification of further requirements, three prototypes differing in their data illustration format were subsequently developed and quantitatively, as well as qualitatively evaluated by conducting an online survey. Thereby, it became evident that future implementations of a corresponding system for patient monitoring ought to integrate a context-dependent data presentation in particular, as this combines high navigability and availability of required data. Identifying patient monitoring during patient transport as a potential context of use, as well as distinguishing a context-dependent design approach as favorable constitute two key contributions of this work and provide a foundation on which future implementations of AR systems in the nursing domain and other related contexts can be established.},\nbooktitle = {Proceedings of the 27th ACM Symposium on Virtual Reality Software and Technology},\narticleno = {11},\nnumpages = {11},\nkeywords = {wearable, critical care, patient monitoring, head-mounted display, augmented reality},\nlocation = {Osaka, Japan},\nseries = {VRST '21}\n}",
            "DOI": "10.1145/3489849.3489852",
            "Authors": [
                "Kimmel",
                "Simon and Cobus",
                "Vanessa and Heuten",
                "Wilko"
            ]
        },
        {
            "Name": "Origin-Destination Flow Maps in Immersive Environments",
            "Opportunity": [
                "Spatial Immersion"
            ],
            "Contribution Type": [
                "Evaluation"
            ],
            "Data Domain": [
                "Migration"
            ],
            "Dataset Types": [
                "Spatial"
            ],
            "Dataset Generation": [
                "Static",
                "Pre-computation"
            ],
            "Presentation": [
                "HMD",
                "VR"
            ],
            "Device": [
                "HTC vive"
            ],
            "Input": [
                "Commercial Controller"
            ],
            "Environment": [
                "Room Scale"
            ],
            "Space": [
                "Finite"
            ],
            "Embodiment": [
                "Egocentric"
            ],
            "Collaboration ": [
                "None"
            ],
            "Interaction": [
                "Grab",
                "Raycast"
            ],
            "Visualization": [
                "3D Origin Destination",
                "3D Globe"
            ],
            "Abstract/Natural": [
                "Abstract"
            ],
            "Manipulate": [
                "Translate",
                "Rotate"
            ],
            "Position": [
                "Unfixed"
            ],
            "Scale": [
                "Medium"
            ],
            "2D or 3D": [
                "3D"
            ],
            "Year": "2019",
            "Bibtex": "@ARTICLE{8440844,\n  author={Yang, Yalong and Dwyer, Tim and Jenny, Bernhard and Marriott, Kim and Cordeil, Maxime and Chen, Haohui},\n  journal={IEEE Transactions on Visualization and Computer Graphics}, \n  title={Origin-Destination Flow Maps in Immersive Environments}, \n  year={2019},\n  volume={25},\n  number={1},\n  pages={693-703},\n  doi={10.1109/TVCG.2018.2865192}}",
            "DOI": "10.1109/TVCG.2018.2865192",
            "Authors": [
                "Y. Yang; T. Dwyer; B. Jenny; K. Marriott; M. Cordeil; H. Chen"
            ]
        },
        {
            "Name": "Personal Augmented Reality for Information Visualization on Large Interactive Displays",
            "Opportunity": [
                "Spatial Immersion"
            ],
            "Contribution Type": [
                "Technique"
            ],
            "Data Domain": [
                "General"
            ],
            "Dataset Types": [
                "Tabular",
                "Network"
            ],
            "Dataset Generation": [
                "Static",
                "Interactive",
                "Pre-computation"
            ],
            "Presentation": [
                "AR",
                "HMD",
                "LID"
            ],
            "Device": [
                "Hololens"
            ],
            "Input": [
                "Gesture"
            ],
            "Environment": [
                "Room Scale",
                "Wall"
            ],
            "Space": [
                "Finite"
            ],
            "Embodiment": [
                "Exocentric"
            ],
            "Collaboration ": [
                "Synchronous Co-located",
                "Cross-virtuality"
            ],
            "Interaction": [
                "Touch",
                "Draw",
                "Grab"
            ],
            "Visualization": [
                "2D Bar Chart",
                "2D Line Chart",
                "2D Pie Chart",
                "Parallel Coord",
                "2D Scatter Plot"
            ],
            "Abstract/Natural": [
                "Abstract"
            ],
            "Manipulate": [
                "Arrange",
                "Annotate"
            ],
            "Position": [
                "Fixed (Point)"
            ],
            "Scale": [
                "Medium"
            ],
            "2D or 3D": [
                "2D"
            ],
            "Year": "2021",
            "Bibtex": "'@ARTICLE {9223669,\nauthor = {P. Reipschlager and T. Flemisch and R. Dachselt},\njournal = {IEEE Transactions on Visualization &amp; Computer Graphics},\ntitle = {Personal Augmented Reality for Information Visualization on Large Interactive Displays},\nyear = {2021},\nvolume = {27},\nnumber = {02},\nissn = {1941-0506},\npages = {1182-1192},\nabstract = {In this work we propose the combination of large interactive displays with personal head-mounted Augmented Reality (AR) for information visualization to facilitate data exploration and analysis. Even though large displays provide more display space, they are challenging with regard to perception, effective multi-user support, and managing data density and complexity. To address these issues and illustrate our proposed setup, we contribute an extensive design space comprising first, the spatial alignment of display, visualizations, and objects in AR space. Next, we discuss which parts of a visualization can be augmented. Finally, we analyze how AR can be used to display personal views in order to show additional information and to minimize the mutual disturbance of data analysts. Based on this conceptual foundation, we present a number of exemplary techniques for extending visualizations with AR and discuss their relation to our design space. We further describe how these techniques address typical visualization problems that we have identified during our literature research. To examine our concepts, we introduce a generic AR visualization framework as well as a prototype implementing several example techniques. In order to demonstrate their potential, we further present a use case walkthrough in which we analyze a movie data set. From these experiences, we conclude that the contributed techniques can be useful in exploring and understanding multivariate data. We are convinced that the extension of large displays with AR for information visualization has a great potential for data analysis and sense-making.},\nkeywords = {data visualization;data analysis;augmented reality;three-dimensional displays;navigation;visualization},\ndoi = {10.1109/TVCG.2020.3030460},\npublisher = {IEEE Computer Society},\naddress = {Los Alamitos, CA, USA},\nmonth = {feb}\n}",
            "DOI": "10.1109/TVCG.2020.3030460",
            "Authors": [
                "P. Reipschlager; T. Flemisch; R. Dachselt"
            ]
        },
        {
            "Name": "Pre-attentive Features in Natural Augmented Reality Visualizations",
            "Opportunity": [
                "Engagement"
            ],
            "Contribution Type": [
                "Evaluation"
            ],
            "Data Domain": [
                "Manufacturing"
            ],
            "Dataset Types": [
                "Tabular"
            ],
            "Dataset Generation": [
                "Static",
                "Pre-computation"
            ],
            "Presentation": [
                "AR",
                "Phone",
                "Handheld"
            ],
            "Device": [
                "NA"
            ],
            "Input": [
                "Mobile Device"
            ],
            "Environment": [
                "Room Scale",
                "Table"
            ],
            "Space": [
                "Situated"
            ],
            "Embodiment": [
                "Exocentric"
            ],
            "Collaboration ": [
                "None"
            ],
            "Interaction": [
                "None"
            ],
            "Visualization": [
                "Glyph/Realism"
            ],
            "Abstract/Natural": [
                "Natural Spatial Mapping"
            ],
            "Manipulate": [
                "None"
            ],
            "Position": [
                "Fixed (Point)"
            ],
            "Scale": [
                "Small"
            ],
            "2D or 3D": [
                "2D"
            ],
            "Year": "2016",
            "Bibtex": "'@INPROCEEDINGS{7836464,\n  author={Barreiros, Carla and Veas, Eduardo and Pammer-Schindler, Viktoria},\n  booktitle={2016 IEEE International Symposium on Mixed and Augmented Reality (ISMAR-Adjunct)}, \n  title={Pre-attentive Features in Natural Augmented Reality Visualizations}, \n  year={2016},\n  volume={},\n  number={},\n  pages={72-73},\n  doi={10.1109/ISMAR-Adjunct.2016.0043}}",
            "DOI": "10.1109/ISMAR-Adjunct.2016.0043",
            "Authors": [
                "C. Barreiros; E. Veas; V. Pammer-Schindler"
            ]
        },
        {
            "Name": "Quantitative Data Visualisation on Virtual Globes",
            "Opportunity": [
                "Spatial Immersion"
            ],
            "Contribution Type": [
                "Evaluation"
            ],
            "Data Domain": [
                "General"
            ],
            "Dataset Types": [
                "Spatial",
                "Tabular"
            ],
            "Dataset Generation": [
                "Static",
                "Pre-computation"
            ],
            "Presentation": [
                "AR",
                "VR",
                "HMD"
            ],
            "Device": [
                "NA"
            ],
            "Input": [
                "Gesture",
                "Mouse/Keyboard"
            ],
            "Environment": [
                "Room Scale"
            ],
            "Space": [
                "Finite"
            ],
            "Embodiment": [
                "Egocentric"
            ],
            "Collaboration ": [
                "None"
            ],
            "Interaction": [
                "Move"
            ],
            "Visualization": [
                "3D Globe",
                "3D Bar Chart",
                "3D Area Chart"
            ],
            "Abstract/Natural": [
                "Abstract",
                "Natural Spatial Mapping"
            ],
            "Manipulate": [
                "Rotate"
            ],
            "Position": [
                "Unfixed"
            ],
            "Scale": [
                "Medium"
            ],
            "2D or 3D": [
                "3D"
            ],
            "Year": "2021",
            "Bibtex": "'@inproceedings{10.1145/3411764.3445152,\nauthor = {Satriadi, Kadek Ananta and Ens, Barrett and Czauderna, Tobias and Cordeil, Maxime and Jenny, Bernhard},\ntitle = {Quantitative Data Visualisation on Virtual Globes},\nyear = {2021},\nisbn = {9781450380966},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3411764.3445152},\ndoi = {10.1145/3411764.3445152},\nabstract = {Geographic data visualisation on virtual globes is intuitive and widespread, but has not been thoroughly investigated. We explore two main design factors for quantitative data visualisation on virtual globes: i)&nbsp;commonly used primitives (2D bar, 3D bar, circle) and ii)&nbsp;the orientation of these primitives (tangential, normal, billboarded). We evaluate five distinctive visualisation idioms in a user study with 50 participants. The results show that aligning primitives tangentially on the globe\u2019s surface decreases the accuracy of area-proportional circle visualisations, while the orientation does not have a significant effect on the accuracy of length-proportional bar visualisations. We also find that tangential primitives induce higher perceived mental load than other orientations. Guided by these results we design a novel globe visualisation idiom, Geoburst, that combines a virtual globe and a radial bar chart. A preliminary evaluation reports potential benefits and drawbacks of the Geoburst visualisation.},\nbooktitle = {Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems},\narticleno = {460},\nnumpages = {14},\nkeywords = {virtual globes, geovisualisation, quantitative data visualisation},\nlocation = {Yokohama, Japan},\nseries = {CHI '21}\n}",
            "DOI": "10.1145/3411764.3445152",
            "Authors": [
                "Satriadi",
                "Kadek Ananta and Ens",
                "Barrett and Czauderna",
                "Tobias and Cordeil",
                "Maxime and Jenny",
                "Bernhard"
            ]
        },
        {
            "Name": "RealitySketch: Embedding Responsive Graphics and Visualizations in AR through Dynamic Sketching",
            "Opportunity": [
                "Engagement"
            ],
            "Contribution Type": [
                "Technique"
            ],
            "Data Domain": [
                "Health",
                "Physical Sciences",
                "Life sciences"
            ],
            "Dataset Types": [
                "Spatial"
            ],
            "Dataset Generation": [
                "Dynamic",
                "Live-computation",
                "Interactive"
            ],
            "Presentation": [
                "AR",
                "Tablet",
                "Handheld"
            ],
            "Device": [
                "iPad Pro 11in 2018"
            ],
            "Input": [
                "Mobile Device"
            ],
            "Environment": [
                "Room Scale",
                "Object"
            ],
            "Space": [
                "Situated"
            ],
            "Embodiment": [
                "Exocentric"
            ],
            "Collaboration ": [
                "Synchronous Co-located",
                "Cross-virtuality"
            ],
            "Interaction": [
                "Touch",
                "Draw",
                "Move",
                "Slide"
            ],
            "Visualization": [
                "2D Line Chart",
                "2D Scatter Plot"
            ],
            "Abstract/Natural": [
                "Abstract",
                "Natural Spatial Mapping"
            ],
            "Manipulate": [
                "Select",
                "Arrange"
            ],
            "Position": [
                "Fixed (Object)"
            ],
            "Scale": [
                "Small",
                "Medium"
            ],
            "2D or 3D": [
                "2D"
            ],
            "Year": "2020",
            "Bibtex": "@inproceedings{10.1145/3379337.3415892,\nauthor = {Suzuki, Ryo and Kazi, Rubaiat Habib and Wei, Li-yi and DiVerdi, Stephen and Li, Wilmot and Leithinger, Daniel},\ntitle = {RealitySketch: Embedding Responsive Graphics and Visualizations in AR through Dynamic Sketching},\nyear = {2020},\nisbn = {9781450375146},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3379337.3415892},\ndoi = {10.1145/3379337.3415892},\nabstract = {We present RealitySketch, an augmented reality interface for sketching interactive graphics and visualizations. In recent years, an increasing number of AR sketching tools enable users to draw and embed sketches in the real world. However, with the current tools, sketched contents are inherently static, floating in mid-air without responding to the real world. This paper introduces a new way to embed dynamic and responsive graphics in the real world. In RealitySketch, the user draws graphical elements on a mobile AR screen and binds them with physical objects in real-time and improvisational ways, so that the sketched elements dynamically move with the corresponding physical motion. The user can also quickly visualize and analyze real-world phenomena through responsive graph plots or interactive visualizations. This paper contributes to a set of interaction techniques that enable capturing, parameterizing, and visualizing real-world motion without pre-defined programs and configurations. Finally, we demonstrate our tool with several application scenarios, including physics education, sports training, and in-situ tangible interfaces.},\nbooktitle = {Proceedings of the 33rd Annual ACM Symposium on User Interface Software and Technology},\npages = {166\u2013181},\nnumpages = {16},\nkeywords = {embedded data visualization, tangible interaction, real-time authoring, augmented reality, sketching interfaces},\nlocation = {Virtual Event, USA},\nseries = {UIST '20}\n}",
            "DOI": "10.1145/3379337.3415892",
            "Authors": [
                "Suzuki",
                "Ryo and Kazi",
                "Rubaiat Habib and Wei",
                "Li-yi and DiVerdi",
                "Stephen and Li",
                "Wilmot and Leithinger",
                "Daniel"
            ]
        },
        {
            "Name": "ReLive: Bridging In-Situ and Ex-Situ Visual Analytics for Analyzing Mixed Reality User Studies",
            "Opportunity": [
                "Spatial Immersion"
            ],
            "Contribution Type": [
                "System"
            ],
            "Data Domain": [
                "Behavioral"
            ],
            "Dataset Types": [
                "Spatial",
                "Tabular"
            ],
            "Dataset Generation": [
                "Static",
                "Pre-computation"
            ],
            "Presentation": [
                "HMD",
                "Desktop"
            ],
            "Device": [
                "Oculus Quest 2"
            ],
            "Input": [
                "Commercial Controller",
                "Mouse/Keyboard"
            ],
            "Environment": [
                "Seated"
            ],
            "Space": [
                "Finite"
            ],
            "Embodiment": [
                "Egocentric"
            ],
            "Collaboration ": [
                "None"
            ],
            "Interaction": [
                "Raycast",
                "Click",
                "Type"
            ],
            "Visualization": [
                "2D Bar Chart",
                "2D Trajectory",
                "3D Trajectory",
                "2D Line Chart",
                "3D Time-Line"
            ],
            "Abstract/Natural": [
                "Natural Spatial Mapping",
                "Abstract"
            ],
            "Manipulate": [
                "Navigate",
                "Select",
                "Filter",
                "Arrange",
                "Change",
                "Annotate"
            ],
            "Position": [
                "Fixed (Point)"
            ],
            "Scale": [
                "Large"
            ],
            "2D or 3D": [
                "3D",
                "2D"
            ],
            "Year": "2022",
            "Bibtex": "@inproceedings{10.1145/3491102.3517550,\nauthor = {Hubenschmid, Sebastian and Wieland, Jonathan and Fink, Daniel Immanuel and Batch, Andrea and Zagermann, Johannes and Elmqvist, Niklas and Reiterer, Harald},\ntitle = {ReLive: Bridging In-Situ and Ex-Situ Visual Analytics for Analyzing Mixed Reality User Studies},\nyear = {2022},\nisbn = {9781450391573},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3491102.3517550},\ndoi = {10.1145/3491102.3517550},\nabstract = {The nascent field of mixed reality is seeing an ever-increasing need for user studies and field evaluation, which are particularly challenging given device heterogeneity, diversity of use, and mobile deployment. Immersive analytics tools have recently emerged to support such analysis in situ, yet the complexity of the data also warrants an ex-situ analysis using more traditional non-immersive visual analytics setups. To bridge the gap between both approaches, we introduce ReLive: a mixed-immersion visual analytics framework for exploring and analyzing mixed reality user studies. ReLive combines an in-situ virtual reality view with a complementary ex-situ desktop view. While the virtual reality view allows users to relive interactive spatial recordings replicating the original study, the synchronized desktop view provides a familiar interface for analyzing aggregated data. We validated our concepts in a two-step evaluation consisting of a design walkthrough and an empirical expert user study.},\nbooktitle = {Proceedings of the 2022 CHI Conference on Human Factors in Computing Systems},\narticleno = {24},\nnumpages = {20},\nkeywords = {virtual reality., Immersive analytics, data visualization, visual analytics},\nlocation = {New Orleans, LA, USA},\nseries = {CHI '22}\n}",
            "DOI": "10.1145/3491102.3517550",
            "Authors": [
                "Hubenschmid",
                "Sebastian and Wieland",
                "Jonathan and Fink",
                "Daniel Immanuel and Batch",
                "Andrea and Zagermann",
                "Johannes and Elmqvist",
                "Niklas and Reiterer",
                "Harald"
            ]
        },
        {
            "Name": "Remote and Collaborative Virtual Reality Experiments via Social VR Platforms",
            "Opportunity": [
                "Collaboration"
            ],
            "Contribution Type": [
                "Evaluation"
            ],
            "Data Domain": [
                "General"
            ],
            "Dataset Types": [
                "Network"
            ],
            "Dataset Generation": [
                "Pre-computation",
                "Static"
            ],
            "Presentation": [
                "HMD",
                "VR"
            ],
            "Device": [
                "NA"
            ],
            "Input": [
                "Commercial Controller"
            ],
            "Environment": [
                "Room Scale",
                "Table"
            ],
            "Space": [
                "Finite"
            ],
            "Embodiment": [
                "Exocentric"
            ],
            "Collaboration ": [
                "Synchronous Distributed"
            ],
            "Interaction": [
                "Grab",
                "Click",
                "Move"
            ],
            "Visualization": [
                "2D Node Link"
            ],
            "Abstract/Natural": [
                "Abstract"
            ],
            "Manipulate": [
                "Change",
                "Translate"
            ],
            "Position": [
                "Fixed (Point)"
            ],
            "Scale": [
                "Medium"
            ],
            "2D or 3D": [
                "2D"
            ],
            "Year": "2021",
            "Bibtex": "'@inproceedings{10.1145/3411764.3445426,\nauthor = {Saffo, David and Di Bartolomeo, Sara and Yildirim, Caglar and Dunne, Cody},\ntitle = {Remote and Collaborative Virtual Reality Experiments via Social VR Platforms},\nyear = {2021},\nisbn = {9781450380966},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3411764.3445426},\ndoi = {10.1145/3411764.3445426},\nabstract = {Virtual reality (VR) researchers struggle to conduct remote studies. Previous work has focused on working around limitations imposed by traditional crowdsourcing methods. However, the potential for leveraging social VR platforms for HCI evaluations is largely unexplored. These platforms have large VR-ready user populations, distributed synchronous virtual environments, and support for user-generated content. We demonstrate how social VR platforms can be used to practically and ethically produce valid research results by replicating two studies using one such platform (VRChat): a quantitative study on Fitts\u2019 Law and a qualitative study on tabletop collaboration. Our replication studies exhibited analogous results to the originals, indicating the research validity of this approach. Moreover, we easily recruited experienced VR users with their own hardware for synchronous, remote, and collaborative participation. We further provide lessons learned for future researchers experimenting using social VR platforms. This paper and all supplemental materials are available at osf.io/c2amz.},\nbooktitle = {Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems},\narticleno = {523},\nnumpages = {15},\nkeywords = {Transferability Study, Crowdsourcing, Virtual Reality, Qualitative study, Replication Study, Social VR, Quantitative Study},\nlocation = {Yokohama, Japan},\nseries = {CHI '21}\n}",
            "DOI": "10.1145/3411764.3445426",
            "Authors": [
                "Saffo",
                "David and Di Bartolomeo",
                "Sara and Yildirim",
                "Caglar and Dunne",
                "Cody"
            ]
        },
        {
            "Name": "Scaptics and Highlight-Planes: Immersive Interaction Techniques for Finding Occluded Features in 3D Scatterplots",
            "Opportunity": [
                "Multi-Sensory Presentation"
            ],
            "Contribution Type": [
                "Technique"
            ],
            "Data Domain": [
                "General"
            ],
            "Dataset Types": [
                "Spatial",
                "Tabular"
            ],
            "Dataset Generation": [
                "Static",
                "Pre-computation"
            ],
            "Presentation": [
                "VR",
                "HMD"
            ],
            "Device": [
                "HTC vive"
            ],
            "Input": [
                "Commercial Controller"
            ],
            "Environment": [
                "Room Scale"
            ],
            "Space": [
                "Finite"
            ],
            "Embodiment": [
                "Exocentric"
            ],
            "Collaboration ": [
                "None"
            ],
            "Interaction": [
                "Brushing"
            ],
            "Visualization": [
                "3D Scatter Plot"
            ],
            "Abstract/Natural": [
                "Abstract"
            ],
            "Manipulate": [
                "Select"
            ],
            "Position": [
                "Fixed (Point)"
            ],
            "Scale": [
                "Medium"
            ],
            "2D or 3D": [
                "3D"
            ],
            "Year": "2019",
            "Bibtex": "'@inproceedings{10.1145/3290605.3300555,\nauthor = {Prouzeau, Arnaud and Cordeil, Maxime and Robin, Clement and Ens, Barrett and Thomas, Bruce H. and Dwyer, Tim},\ntitle = {Scaptics and Highlight-Planes: Immersive Interaction Techniques for Finding Occluded Features in 3D Scatterplots},\nyear = {2019},\nisbn = {9781450359702},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3290605.3300555},\ndoi = {10.1145/3290605.3300555},\nabstract = {Three-dimensional scatterplots suffer from well-known perception and usability problems. In particular, overplotting and occlusion, mainly due to density and noise, prevent users from properly perceiving the data. Thanks to accurate head and hand tracking, immersive Virtual Reality (VR) setups provide new ways to interact and navigate with 3D scatterplots. VR also supports additional sensory modalities such as haptic feedback. Inspired by methods commonly used in Scientific Visualisation to visually explore volumes, we propose two techniques that leverage the immersive aspects of VR: first, a density-based haptic vibration technique (Scaptics) which provides feedback through the controller; and second, an adaptation of a cutting plane for 3D scatterplots (Highlight-Plane). We evaluated both techniques in a controlled study with two tasks involving density (finding high- and low-density areas). Overall, Scaptics was the most time-efficient and accurate technique, however, in some conditions, it was outperformed by Highlight-Plane.},\nbooktitle = {Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems},\npages = {1\u201312},\nnumpages = {12},\nkeywords = {virtual reality, 3d scatterplot, haptic, vibrotactile feedback},\nlocation = {Glasgow, Scotland Uk},\nseries = {CHI '19}\n}",
            "DOI": "10.1145/3290605.3300555",
            "Authors": [
                "Prouzeau",
                "Arnaud and Cordeil",
                "Maxime and Robin",
                "Clement and Ens",
                "Barrett and Thomas",
                "Bruce H. and Dwyer",
                "Tim"
            ]
        },
        {
            "Name": "SecondSight: A Framework for Cross-Device Augmented Reality Interfaces",
            "Opportunity": [
                "Spatial Immersion"
            ],
            "Contribution Type": [
                "Technique"
            ],
            "Data Domain": [
                "Health",
                "Migration",
                "Life sciences"
            ],
            "Dataset Types": [
                "Tabular"
            ],
            "Dataset Generation": [
                "Static"
            ],
            "Presentation": [
                "AR",
                "HMD",
                "Phone"
            ],
            "Device": [
                "Metavision Meta 2 glasses"
            ],
            "Input": [
                "Gaze",
                "Mobile Device"
            ],
            "Environment": [
                "Room Scale"
            ],
            "Space": [
                "Situated"
            ],
            "Embodiment": [
                "Egocentric"
            ],
            "Collaboration ": [
                "None"
            ],
            "Interaction": [
                "Touch",
                "Pinch",
                "Move",
                "Gesture",
                "Gaze"
            ],
            "Visualization": [
                "2D Bar Chart"
            ],
            "Abstract/Natural": [
                "Abstract"
            ],
            "Manipulate": [
                "Arrange"
            ],
            "Position": [
                "Unfixed",
                "Fixed (Object)"
            ],
            "Scale": [
                "Small"
            ],
            "2D or 3D": [
                "2D"
            ],
            "Year": "2021",
            "Bibtex": "'@inproceedings{10.1145/3411763.3451839,\nauthor = {Reichherzer, Carolin and Fraser, Jack and Rompapas, Damien Constantine and Billinghurst, Mark},\ntitle = {SecondSight: A Framework for Cross-Device Augmented Reality Interfaces},\nyear = {2021},\nisbn = {9781450380959},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3411763.3451839},\ndoi = {10.1145/3411763.3451839},\nabstract = {This paper describes a modular framework developed to facilitate the design space exploration of cross-device Augmented Reality (AR) interfaces that combine an AR head-mounted display (HMD) with a smartphone. Currently, there is a growing interest in how AR HMDs can be used with smartphones to improve the user\u2019s AR experience. In this work, we describe a framework that enables rapid prototyping and evaluation of an interface. Our system enables different modes of interaction, content placement, and simulated AR HMD field of view to assess which combination is best suited to inform future researchers on design recommendations. We provide examples of how the framework could be used to create sample applications, the types of the studies which could be supported, and example results from a simple pilot study.},\nbooktitle = {Extended Abstracts of the 2021 CHI Conference on Human Factors in Computing Systems},\narticleno = {234},\nnumpages = {6},\nkeywords = {cross-device interaction, smartphone, interaction methods, field of view, Augmented Reality},\nlocation = {Yokohama, Japan},\nseries = {CHI EA '21}\n}",
            "DOI": "10.1145/3411763.3451839",
            "Authors": [
                "Reichherzer",
                "Carolin and Fraser",
                "Jack and Rompapas",
                "Damien Constantine and Billinghurst",
                "Mark"
            ]
        },
        {
            "Name": "Shared Surfaces and Spaces: Collaborative Data Visualisation in a Co-located Immersive Environment",
            "Opportunity": [
                "Collaboration"
            ],
            "Contribution Type": [
                "Evaluation"
            ],
            "Data Domain": [
                "General"
            ],
            "Dataset Types": [
                "Tabular"
            ],
            "Dataset Generation": [
                "Static",
                "Pre-computation"
            ],
            "Presentation": [
                "VR",
                "HMD"
            ],
            "Device": [
                "HTC Vive pro"
            ],
            "Input": [
                "Commercial Controller"
            ],
            "Environment": [
                "Room Scale",
                "Table",
                "Wall"
            ],
            "Space": [
                "Finite"
            ],
            "Embodiment": [
                "Exocentric"
            ],
            "Collaboration ": [
                "Synchronous Co-located"
            ],
            "Interaction": [
                "Grab",
                "Raycast"
            ],
            "Visualization": [
                "2D Line Chart",
                "2D Scatter Plot",
                "3D Scatter Plot"
            ],
            "Abstract/Natural": [
                "Abstract"
            ],
            "Manipulate": [
                "Change",
                "Translate",
                "Scale",
                "Filter",
                "Arrange"
            ],
            "Position": [
                "Unfixed",
                "Fixed (Point)"
            ],
            "Scale": [
                "Medium",
                "Small"
            ],
            "2D or 3D": [
                "3D",
                "2D"
            ],
            "Year": "2021",
            "Bibtex": "@ARTICLE{9222346,\n  author={Lee, Benjamin and Hu, Xiaoyun and Cordeil, Maxime and Prouzeau, Arnaud and Jenny, Bernhard and Dwyer, Tim},\n  journal={IEEE Transactions on Visualization and Computer Graphics}, \n  title={Shared Surfaces and Spaces: Collaborative Data Visualisation in a Co-located Immersive Environment}, \n  year={2021},\n  volume={27},\n  number={2},\n  pages={1171-1181},\n  doi={10.1109/TVCG.2020.3030450}}",
            "DOI": "10.1109/TVCG.2020.3030450",
            "Authors": [
                "B. Lee; X. Hu; M. Cordeil; A. Prouzeau; B. Jenny; T. Dwyer"
            ]
        },
        {
            "Name": "Shvil: Collaborative Augmented Reality Land Navigation",
            "Opportunity": [
                "Collaboration"
            ],
            "Contribution Type": [
                "System"
            ],
            "Data Domain": [
                "PNT"
            ],
            "Dataset Types": [
                "Spatial"
            ],
            "Dataset Generation": [
                "Live-computation",
                "Dynamic",
                "Pre-computation",
                "Interactive"
            ],
            "Presentation": [
                "AR",
                "Tablet",
                "Handheld"
            ],
            "Device": [
                "iPad Air",
                "Lenovo Tablet"
            ],
            "Input": [
                "Mobile Device"
            ],
            "Environment": [
                "Outdoors",
                "Room Scale",
                "Object"
            ],
            "Space": [
                "Situated"
            ],
            "Embodiment": [
                "Exocentric"
            ],
            "Collaboration ": [
                "Synchronous Distributed"
            ],
            "Interaction": [
                "Touch",
                "Move"
            ],
            "Visualization": [
                "3D Trajectory",
                "2D Map",
                "3D Map"
            ],
            "Abstract/Natural": [
                "Natural Spatial Mapping"
            ],
            "Manipulate": [
                "Navigate"
            ],
            "Position": [
                "Fixed (Point)",
                "Fixed (Object)"
            ],
            "Scale": [
                "Small",
                "Large"
            ],
            "2D or 3D": [
                "2D",
                "3D"
            ],
            "Year": "2014",
            "Bibtex": "'@inproceedings{10.1145/2559206.2581147,\nauthor = {Li, Nico and Nittala, Aditya Shekhar and Sharlin, Ehud and Costa Sousa, Mario},\ntitle = {Shvil: Collaborative Augmented Reality Land Navigation},\nyear = {2014},\nisbn = {9781450324748},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/2559206.2581147},\ndoi = {10.1145/2559206.2581147},\nabstract = {We present our prototype of Shvil, an Augmented Reality (AR) system for collaborative land navigation. Shvil facilitates path planning and execution by creating a collaborative medium between an overseer (indoor user) and an explorer (outdoor user) using AR and 3D printing techniques. Shvil provides a remote overseer with a physical representation of the topography of the mission, and merges the physical presence of the explorer and the actions of the overseer via dynamic AR visualization. The system supports collaboration by both overlaying visual information related to the explorer on top of the overseer's local physical representation, and overlaying visual information in-situ for the explorer as it emerges from the overseer. We report our current prototype effort and preliminary results, and our vision for the future of Shvil.},\nbooktitle = {CHI '14 Extended Abstracts on Human Factors in Computing Systems},\npages = {1291\u20131296},\nnumpages = {6},\nkeywords = {computer supported cooperative work (cscw), visualization, augmented reality and tangible ui, mixed reality, route finding},\nlocation = {Toronto, Ontario, Canada},\nseries = {CHI EA '14}\n}",
            "DOI": "10.1145/2559206.2581147",
            "Authors": [
                "Li",
                "Nico and Nittala",
                "Aditya Shekhar and Sharlin",
                "Ehud and Costa Sousa",
                "Mario"
            ]
        },
        {
            "Name": "SPARVIS: Combining Smartphone and Augmented Reality for Visual Data Analytics",
            "Opportunity": [
                "Spatial Immersion"
            ],
            "Contribution Type": [
                "System"
            ],
            "Data Domain": [
                "General"
            ],
            "Dataset Types": [
                "Tabular",
                "Spatial"
            ],
            "Dataset Generation": [
                "Static",
                "Pre-computation"
            ],
            "Presentation": [
                "AR",
                "HMD"
            ],
            "Device": [
                "NA"
            ],
            "Input": [
                "Mobile Device"
            ],
            "Environment": [
                "Room Scale"
            ],
            "Space": [
                "Finite"
            ],
            "Embodiment": [
                "Egocentric"
            ],
            "Collaboration ": [
                "None"
            ],
            "Interaction": [
                "Raycast",
                "Touch"
            ],
            "Visualization": [
                "2D Bar Chart",
                "2D Pie Chart",
                "2D Line Chart",
                "2D Map"
            ],
            "Abstract/Natural": [
                "Abstract",
                "Natural Spatial Mapping"
            ],
            "Manipulate": [
                "Change",
                "Filter",
                "Arrange",
                "Translate",
                "Zoom",
                "Select"
            ],
            "Position": [
                "Unfixed"
            ],
            "Scale": [
                "Medium"
            ],
            "2D or 3D": [
                "2D"
            ],
            "Year": "2022",
            "Bibtex": "'@INPROCEEDINGS{9974220,\nauthor={Huang, Jinbin and Liang, Shuang and Xiong, Qi and Gao, Yu and Mei, Chao and Xu, Yi and Bryan, Chris},\nbooktitle={2022 IEEE International Symposium on Mixed and Augmented Reality Adjunct (ISMAR-Adjunct)},\ntitle={SPARVIS: Combining Smartphone and Augmented Reality for Visual Data Analytics},\nyear={2022},\nvolume={},\nnumber={},\npages={111-117},\ndoi={10.1109/ISMAR-Adjunct57072.2022.00030}}",
            "DOI": "10.1109/ISMAR-Adjunct57072.2022.00030",
            "Authors": [
                "Huang",
                "Jinbin and Liang",
                "Shuang and Xiong",
                "Qi and Gao",
                "Yu and Mei",
                "Chao and Xu",
                "Yi and Bryan",
                "Chris"
            ]
        },
        {
            "Name": "STREAM: Exploring the Combination of Spatially-Aware Tablets with Augmented Reality Head-Mounted Displays for Immersive Analytics",
            "Opportunity": [
                "Spatial Immersion"
            ],
            "Contribution Type": [
                "System"
            ],
            "Data Domain": [
                "General"
            ],
            "Dataset Types": [
                "Tabular"
            ],
            "Dataset Generation": [
                "Static",
                "Live-computation"
            ],
            "Presentation": [
                "HMD",
                "Tablet"
            ],
            "Device": [
                "Hololens"
            ],
            "Input": [
                "Custom Controller",
                "Mobile Device"
            ],
            "Environment": [
                "Room Scale",
                "Object"
            ],
            "Space": [
                "Finite"
            ],
            "Embodiment": [
                "Exocentric"
            ],
            "Collaboration ": [
                "None"
            ],
            "Interaction": [
                "Touch",
                "Move"
            ],
            "Visualization": [
                "3D Parallel Coord",
                "Area Chart",
                "2D Scatter Plot"
            ],
            "Abstract/Natural": [
                "Abstract"
            ],
            "Manipulate": [
                "Select",
                "Change",
                "Arrange",
                "Filter"
            ],
            "Position": [
                "Unfixed",
                "Fixed (Object)"
            ],
            "Scale": [
                "Large",
                "Small"
            ],
            "2D or 3D": [
                "3D",
                "2D"
            ],
            "Year": "2021",
            "Bibtex": "'@inproceedings{10.1145/3411764.3445298,\nauthor = {Hubenschmid, Sebastian and Zagermann, Johannes and Butscher, Simon and Reiterer, Harald},\ntitle = {STREAM: Exploring the Combination of Spatially-Aware Tablets with Augmented Reality Head-Mounted Displays for Immersive Analytics},\nyear = {2021},\nisbn = {9781450380966},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3411764.3445298},\ndoi = {10.1145/3411764.3445298},\nabstract = {Recent research in the area of immersive analytics demonstrated the utility of head-mounted augmented reality devices for visual data analysis. However, it can be challenging to use the by default supported mid-air gestures to interact with visualizations in augmented reality (e.g. due to limited precision). Touch-based interaction (e.g. via mobile devices) can compensate for these drawbacks, but is limited to two-dimensional input. In this work we present STREAM: Spatially-aware Tablets combined with Augmented Reality Head-Mounted Displays for the multimodal interaction with 3D visualizations. We developed a novel eyes-free interaction concept for the seamless transition between the tablet and the augmented reality environment. A user study reveals that participants appreciated the novel interaction concept, indicating the potential for spatially-aware tablets in augmented reality. Based on our findings, we provide design insights to foster the application of spatially-aware touch devices in augmented reality and research implications indicating areas that need further investigation.},\nbooktitle = {Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems},\narticleno = {469},\nnumpages = {14},\nkeywords = {immersive analytics, augmented reality, visualizations, mobile devices, multimodal interaction},\nlocation = {Yokohama, Japan},\nseries = {CHI '21}\n}",
            "DOI": "10.1145/3411764.3445298",
            "Authors": [
                "Hubenschmid",
                "Sebastian and Zagermann",
                "Johannes and Butscher",
                "Simon and Reiterer",
                "Harald"
            ]
        },
        {
            "Name": "Tangible Globes for Data Visualisation in Augmented Reality",
            "Opportunity": [
                "Embodied Data Exploration"
            ],
            "Contribution Type": [
                "Technique"
            ],
            "Data Domain": [
                "General"
            ],
            "Dataset Types": [
                "Tabular",
                "Spatial"
            ],
            "Dataset Generation": [
                "Pre-computation",
                "Static"
            ],
            "Presentation": [
                "HMD",
                "AR"
            ],
            "Device": [
                "Hololens"
            ],
            "Input": [
                "Custom Controller",
                "Gesture"
            ],
            "Environment": [
                "Room Scale"
            ],
            "Space": [
                "Finite"
            ],
            "Embodiment": [
                "Egocentric",
                "Exocentric"
            ],
            "Collaboration ": [
                "None"
            ],
            "Interaction": [
                "Grab",
                "Rotate",
                "Touch",
                "Gesture"
            ],
            "Visualization": [
                "3D Globe",
                "2D Bar Chart",
                "Point Map",
                "2D Scatter Plot",
                "3D Origin Destination",
                "2D Map"
            ],
            "Abstract/Natural": [
                "Abstract",
                "Natural Spatial Mapping"
            ],
            "Manipulate": [
                "Translate",
                "Rotate",
                "Arrange",
                "Change",
                "Select"
            ],
            "Position": [
                "Fixed (Object)",
                "Fixed (Point)"
            ],
            "Scale": [
                "Medium",
                "Small",
                "Large"
            ],
            "2D or 3D": [
                "3D",
                "2D"
            ],
            "Year": "2022",
            "Bibtex": "'@inproceedings{10.1145/3491102.3517715,\nauthor = {Satriadi, Kadek Ananta and Smiley, Jim and Ens, Barrett and Cordeil, Maxime and Czauderna, Tobias and Lee, Benjamin and Yang, Ying and Dwyer, Tim and Jenny, Bernhard},\ntitle = {Tangible Globes for Data Visualisation in Augmented Reality},\nyear = {2022},\nisbn = {9781450391573},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3491102.3517715},\ndoi = {10.1145/3491102.3517715},\nabstract = {Head-mounted augmented reality (AR) displays allow for the seamless integration of virtual visualisation with contextual tangible references, such as physical (tangible) globes. We explore the design of immersive geospatial data visualisation with AR and tangible globes. We investigate the \u201ctangible-virtual interplay\u201d of tangible globes with virtual data visualisation, and propose a conceptual approach for designing immersive geospatial globes. We demonstrate a set of use cases, such as augmenting a tangible globe with virtual overlays, using a physical globe as a tangible input device for interacting with virtual globes and maps, and linking an augmented globe to an abstract data visualisation. We gathered qualitative feedback from experts about our use case visualisations, and compiled a summary of key takeaways as well as ideas for envisioned future improvements. The proposed design space, example visualisations and lessons learned aim to guide the design of tangible globes for data visualisation in AR.},\nbooktitle = {Proceedings of the 2022 CHI Conference on Human Factors in Computing Systems},\narticleno = {505},\nnumpages = {16},\nkeywords = {immersive analytics, tangible user interface, geographic visualisation, augmented reality},\nlocation = {New Orleans, LA, USA},\nseries = {CHI '22}\n}",
            "DOI": "10.1145/3491102.3517715",
            "Authors": [
                "Satriadi",
                "Kadek Ananta and Smiley",
                "Jim and Ens",
                "Barrett and Cordeil",
                "Maxime and Czauderna",
                "Tobias and Lee",
                "Benjamin and Yang",
                "Ying and Dwyer",
                "Tim and Jenny",
                "Bernhard"
            ]
        },
        {
            "Name": "TangibleData: Interactive Data Visualization with Mid-Air Haptics",
            "Opportunity": [
                "Multi-Sensory Presentation"
            ],
            "Contribution Type": [
                "Technique"
            ],
            "Data Domain": [
                "Biology",
                "Life sciences"
            ],
            "Dataset Types": [
                "Volumetric data",
                "Tabular"
            ],
            "Dataset Generation": [
                "Static",
                "Pre-computation"
            ],
            "Presentation": [
                "VR",
                "HMD",
                "Haptic Display"
            ],
            "Device": [
                "Oculus Rift CV1"
            ],
            "Input": [
                "Gesture"
            ],
            "Environment": [
                "Seated"
            ],
            "Space": [
                "Infinite"
            ],
            "Embodiment": [
                "Egocentric"
            ],
            "Collaboration ": [
                "None"
            ],
            "Interaction": [
                "Pinch",
                "Touch",
                "Gesture"
            ],
            "Visualization": [
                "Volume Visualization",
                "3D Scatter Plot",
                "Anatomical"
            ],
            "Abstract/Natural": [
                "Abstract",
                "Natural Spatial Mapping"
            ],
            "Manipulate": [
                "Translate",
                "Rotate",
                "Scale",
                "Arrange"
            ],
            "Position": [
                "Unfixed",
                "Fixed (Body)"
            ],
            "Scale": [
                "Medium"
            ],
            "2D or 3D": [
                "3D"
            ],
            "Year": "2021",
            "Bibtex": "@inproceedings{10.1145/3489849.3489890,\nauthor = {Bhardwaj, Ayush and Chae, Junghoon and Noeske, Richard Huynh and Kim, Jin Ryong},\ntitle = {TangibleData: Interactive Data Visualization with Mid-Air Haptics},\nyear = {2021},\nisbn = {9781450390927},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3489849.3489890},\ndoi = {10.1145/3489849.3489890},\nabstract = {In this paper, we investigate the effects of mid-air haptics in interactive 3D data visualization. We build an interactive 3D data visualization tool that adapts hand gestures and mid-air haptics to provide tangible interaction in VR using ultrasound haptic feedback on 3D data visualization. We consider two types of 3D visualization datasets and provide different data encoding methods for haptic representations. Two user experiments are conducted to evaluate the effectiveness of our approach. The first experimental results show that adding a mid-air haptic modality can be beneficial regardless of noise conditions and useful for handling occlusion or discerning density and volume information. The second experiment results further show the strengths and weaknesses of direct touch and indirect touch modes. Our findings can shed light on designing and implementing a tangible interaction on 3D data visualization with mid-air haptic feedback.},\nbooktitle = {Proceedings of the 27th ACM Symposium on Virtual Reality Software and Technology},\narticleno = {29},\nnumpages = {11},\nkeywords = {immersive analytics, haptics, Data visualization, virtual reality},\nlocation = {Osaka, Japan},\nseries = {VRST '21}\n}",
            "DOI": "10.1145/3489849.3489890",
            "Authors": [
                "Bhardwaj",
                "Ayush and Chae",
                "Junghoon and Noeske",
                "Richard Huynh and Kim",
                "Jin Ryong"
            ]
        },
        {
            "Name": "The Hologram in My Hand: How Effective is Interactive Exploration of 3D Visualizations in Immersive Tangible Augmented Reality?",
            "Opportunity": [
                "Embodied Data Exploration"
            ],
            "Contribution Type": [
                "Evaluation"
            ],
            "Data Domain": [
                "Biology",
                "Physical Sciences",
                "Life sciences"
            ],
            "Dataset Types": [
                "Tabular"
            ],
            "Dataset Generation": [
                "Static",
                "Pre-computation"
            ],
            "Presentation": [
                "HMD",
                "AR",
                "Desktop",
                "Tablet"
            ],
            "Device": [
                "Hololens"
            ],
            "Input": [
                "Custom Controller",
                "Mouse/Keyboard"
            ],
            "Environment": [
                "Seated"
            ],
            "Space": [
                "Finite"
            ],
            "Embodiment": [
                "Exocentric"
            ],
            "Collaboration ": [
                "None"
            ],
            "Interaction": [
                "Move",
                "Click",
                "Type"
            ],
            "Visualization": [
                "3D Scatter Plot"
            ],
            "Abstract/Natural": [
                "Abstract"
            ],
            "Manipulate": [
                "Slice",
                "Rotate",
                "Select"
            ],
            "Position": [
                "Fixed (Point)"
            ],
            "Scale": [
                "Small"
            ],
            "2D or 3D": [
                "3D"
            ],
            "Year": "2018",
            "Bibtex": "@ARTICLE{8019876,\n  author={Bach, Benjamin and Sicat, Ronell and Beyer, Johanna and Cordeil, Maxime and Pfister, Hanspeter},\n  journal={IEEE Transactions on Visualization and Computer Graphics}, \n  title={The Hologram in My Hand: How Effective is Interactive Exploration of 3D Visualizations in Immersive Tangible Augmented Reality?}, \n  year={2018},\n  volume={24},\n  number={1},\n  pages={457-467},\n  doi={10.1109/TVCG.2017.2745941}}",
            "DOI": "10.1109/TVCG.2017.2745941",
            "Authors": [
                "B. Bach; R. Sicat; J. Beyer; M. Cordeil; H. Pfister"
            ]
        },
        {
            "Name": "The Impact of Immersion on Cluster Identification Tasks",
            "Opportunity": [
                "Spatial Immersion"
            ],
            "Contribution Type": [
                "Evaluation"
            ],
            "Data Domain": [
                "General"
            ],
            "Dataset Types": [
                "Tabular"
            ],
            "Dataset Generation": [
                "Static",
                "Pre-computation"
            ],
            "Presentation": [
                "Desktop",
                "VR",
                "HMD"
            ],
            "Device": [
                "HTC vive"
            ],
            "Input": [
                "Commercial Controller",
                "Mouse/Keyboard"
            ],
            "Environment": [
                "Seated",
                "Table",
                "Room Scale"
            ],
            "Space": [
                "Finite",
                "Infinite"
            ],
            "Embodiment": [
                "Egocentric",
                "Exocentric"
            ],
            "Collaboration ": [
                "None"
            ],
            "Interaction": [
                "Raycast"
            ],
            "Visualization": [
                "3D Scatter Plot",
                "2D Scatter Plot"
            ],
            "Abstract/Natural": [
                "Abstract"
            ],
            "Manipulate": [
                "Select"
            ],
            "Position": [
                "Fixed (Point)",
                "Unfixed"
            ],
            "Scale": [
                "Medium",
                "Large"
            ],
            "2D or 3D": [
                "3D",
                "2D"
            ],
            "Year": "2020",
            "Bibtex": "@ARTICLE{8836087,\n  author={Kraus, M. and Weiler, N. and Oelke, D. and Kehrer, J. and Keim, D. A. and Fuchs, J.},\n  journal={IEEE Transactions on Visualization and Computer Graphics}, \n  title={The Impact of Immersion on Cluster Identification Tasks}, \n  year={2020},\n  volume={26},\n  number={1},\n  pages={525-535},\n  doi={10.1109/TVCG.2019.2934395}}",
            "DOI": "10.1109/TVCG.2019.2934395",
            "Authors": [
                "M. Kraus; N. Weiler; D. Oelke; J. Kehrer; D. A. Keim; J. Fuchs"
            ]
        },
        {
            "Name": "Touch and Beyond: Comparing Physical and Virtual Reality Visualizations",
            "Opportunity": [
                "Spatial Immersion"
            ],
            "Contribution Type": [
                "Evaluation"
            ],
            "Data Domain": [
                "General"
            ],
            "Dataset Types": [
                "Tabular"
            ],
            "Dataset Generation": [
                "Pre-computation",
                "Static"
            ],
            "Presentation": [
                "HMD",
                "VR"
            ],
            "Device": [
                "HTC vive"
            ],
            "Input": [
                "Custom Controller",
                "Commercial Controller",
                "Mobile Device"
            ],
            "Environment": [
                "Room Scale",
                "Object"
            ],
            "Space": [
                "Finite"
            ],
            "Embodiment": [
                "Exocentric"
            ],
            "Collaboration ": [
                "None"
            ],
            "Interaction": [
                "Draw",
                "Touch",
                "Move",
                "Rotate"
            ],
            "Visualization": [
                "3D Bar Chart"
            ],
            "Abstract/Natural": [
                "Abstract"
            ],
            "Manipulate": [
                "Annotate",
                "Filter",
                "Translate",
                "Rotate",
                "Select"
            ],
            "Position": [
                "Fixed (Point)",
                "Fixed (Body)"
            ],
            "Scale": [
                "Small",
                "Medium"
            ],
            "2D or 3D": [
                "3D"
            ],
            "Year": "2022",
            "Bibtex": "@ARTICLE{9193986,\n  author={Danyluk, Kurtis and Ulusoy, Teoman and Wei, Wei and Willett, Wesley},\n  journal={IEEE Transactions on Visualization and Computer Graphics}, \n  title={Touch and Beyond: Comparing Physical and Virtual Reality Visualizations}, \n  year={2022},\n  volume={28},\n  number={4},\n  pages={1930-1940},\n  doi={10.1109/TVCG.2020.3023336}}",
            "DOI": "10.1109/TVCG.2020.3023336",
            "Authors": [
                "Kurtis Danyluk; Teoman Ulusoy; Wei Wei; Wesley Willett"
            ]
        },
        {
            "Name": "Toward Agile Situated Visualization: An Exploratory User Study",
            "Opportunity": [
                "Spatial Immersion"
            ],
            "Contribution Type": [
                "Evaluation"
            ],
            "Data Domain": [
                "General"
            ],
            "Dataset Types": [
                "Tabular"
            ],
            "Dataset Generation": [
                "Static",
                "Pre-computation"
            ],
            "Presentation": [
                "HMD",
                "AR"
            ],
            "Device": [
                "Hololens"
            ],
            "Input": [
                "Gesture",
                "Gaze",
                "Mouse/Keyboard"
            ],
            "Environment": [
                "Room Scale",
                "Seated"
            ],
            "Space": [
                "Situated"
            ],
            "Embodiment": [
                "Egocentric"
            ],
            "Collaboration ": [
                "None"
            ],
            "Interaction": [
                "Pinch",
                "Gaze"
            ],
            "Visualization": [
                "Space-Time Cube"
            ],
            "Abstract/Natural": [
                "Abstract"
            ],
            "Manipulate": [
                "Select",
                "Translate",
                "Arrange"
            ],
            "Position": [
                "Unfixed"
            ],
            "Scale": [
                "Medium",
                "Large"
            ],
            "2D or 3D": [
                "3D"
            ],
            "Year": "2020",
            "Bibtex": "'@inproceedings{10.1145/3334480.3383017,\nauthor = {Merino, Leonel and Sotomayor-G\\'{o}mez, Boris and Yu, Xingyao and Salgado, Ronie and Bergel, Alexandre and Sedlmair, Michael and Weiskopf, Daniel},\ntitle = {Toward Agile Situated Visualization: An Exploratory User Study},\nyear = {2020},\nisbn = {9781450368193},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3334480.3383017},\ndoi = {10.1145/3334480.3383017},\nabstract = {We introduce AVAR, a prototypical implementation of an agile situated visualization (SV) toolkit targeting liveness, integration, and expressiveness. We report on results of an exploratory study with AVAR and seven expert users. In it, participants wore a Microsoft HoloLens device and used a Bluetooth keyboard to program a visualization script for a given dataset. To support our analysis, we (i) video recorded sessions, (ii) tracked users' interactions, and (iii) collected data of participants' impressions. Our prototype confirms that agile SV is feasible. That is, liveness boosted participants' engagement when programming an SV, and so, the sessions were highly interactive and participants were willing to spend much time using our toolkit (i.e., median \u2265 1.5 hours). Participants used our integrated toolkit to deal with data transformations, visual mappings, and view transformations without leaving the immersive environment. Finally, participants benefited from our expressive toolkit and employed multiple of the available features when programming an SV.},\nbooktitle = {Extended Abstracts of the 2020 CHI Conference on Human Factors in Computing Systems},\npages = {1\u20137},\nnumpages = {7},\nkeywords = {augmented reality, user study, situated visualization},\nlocation = {Honolulu, HI, USA},\nseries = {CHI EA '20}\n}",
            "DOI": "10.1145/3334480.3383017",
            "Authors": [
                "Merino",
                "Leonel and Sotomayor-G\\'{o}mez",
                "Boris and Yu",
                "Xingyao and Salgado",
                "Ronie and Bergel",
                "Alexandre and Sedlmair",
                "Michael and Weiskopf",
                "Daniel"
            ]
        },
        {
            "Name": "Towards an Understanding of Augmented Reality Extensions for Existing 3D Data Analysis Tools",
            "Opportunity": [
                "Spatial Immersion"
            ],
            "Contribution Type": [
                "Design Study"
            ],
            "Data Domain": [
                "Physical Sciences"
            ],
            "Dataset Types": [
                "Tabular",
                "Spatial"
            ],
            "Dataset Generation": [
                "Static",
                "Pre-computation"
            ],
            "Presentation": [
                "HMD",
                "AR"
            ],
            "Device": [
                "Hololens"
            ],
            "Input": [
                "Mouse/Keyboard"
            ],
            "Environment": [
                "Seated",
                "Table"
            ],
            "Space": [
                "Situated"
            ],
            "Embodiment": [
                "Egocentric"
            ],
            "Collaboration ": [
                "None"
            ],
            "Interaction": [
                "Click",
                "Draw",
                "Brushing",
                "Type"
            ],
            "Visualization": [
                "2D Histogram",
                "3D Trajectory"
            ],
            "Abstract/Natural": [
                "Abstract",
                "Natural Spatial Mapping"
            ],
            "Manipulate": [
                "Select",
                "Zoom",
                "Arrange",
                "Translate",
                "Filter",
                "Scale",
                "Rotate"
            ],
            "Position": [
                "Fixed (Point)"
            ],
            "Scale": [
                "Small"
            ],
            "2D or 3D": [
                "2D",
                "3D"
            ],
            "Year": "2020",
            "Bibtex": "'@inproceedings{10.1145/3313831.3376657,\nauthor = {Wang, Xiyao and Besan\\c{c}on, Lonni and Rousseau, David and Sereno, Mickael and Ammi, Mehdi and Isenberg, Tobias},\ntitle = {Towards an Understanding of Augmented Reality Extensions for Existing 3D Data Analysis Tools},\nyear = {2020},\nisbn = {9781450367080},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3313831.3376657},\ndoi = {10.1145/3313831.3376657},\nabstract = {We present an observational study with domain experts to understand how augmented reality (AR) extensions to traditional PC-based data analysis tools can help particle physicists to explore and understand 3D data. Our goal is to allow researchers to integrate stereoscopic AR-based visual representations and interaction techniques into their tools, and thus ultimately to increase the adoption of modern immersive analytics techniques in existing data analysis workflows. We use Microsoft's HoloLens as a lightweight and easily maintainable AR headset and replicate existing visualization and interaction capabilities on both the PC and the AR view. We treat the AR headset as a second yet stereoscopic screen, allowing researchers to study their data in a connected multi-view manner. Our results indicate that our collaborating physicists appreciate a hybrid data exploration setup with an interactive AR extension to improve their understanding of particle collision events.},\nbooktitle = {Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems},\npages = {1\u201313},\nnumpages = {13},\nkeywords = {3D visualization, hybrid visualization system, immersive analytics, user interface},\nlocation = {Honolulu, HI, USA},\nseries = {CHI '20}\n}",
            "DOI": "10.1145/3313831.3376657",
            "Authors": [
                "Wang",
                "Xiyao and Besan\\c{c}on",
                "Lonni and Rousseau",
                "David and Sereno",
                "Mickael and Ammi",
                "Mehdi and Isenberg",
                "Tobias"
            ]
        },
        {
            "Name": "Towards an Understanding of Situated AR Visualization for Basketball Free-Throw Training",
            "Opportunity": [
                "Situated Analytics"
            ],
            "Contribution Type": [
                "Evaluation"
            ],
            "Data Domain": [
                "Health",
                "Physical Sciences",
                "Life sciences"
            ],
            "Dataset Types": [
                "Spatial"
            ],
            "Dataset Generation": [
                "Dynamic",
                "Live-computation"
            ],
            "Presentation": [
                "HMD",
                "AR"
            ],
            "Device": [
                "Hololens"
            ],
            "Input": [
                "Body",
                "Mouse/Keyboard"
            ],
            "Environment": [
                "Room Scale"
            ],
            "Space": [
                "Situated"
            ],
            "Embodiment": [
                "Egocentric"
            ],
            "Collaboration ": [
                "None"
            ],
            "Interaction": [
                "Move"
            ],
            "Visualization": [
                "2D Trajectory",
                "3D Trajectory"
            ],
            "Abstract/Natural": [
                "Natural Spatial Mapping"
            ],
            "Manipulate": [
                "Change"
            ],
            "Position": [
                "Unfixed"
            ],
            "Scale": [
                "Medium"
            ],
            "2D or 3D": [
                "2D",
                "3D"
            ],
            "Year": "2021",
            "Bibtex": "@inproceedings{10.1145/3411764.3445649,\nauthor = {Lin, Tica and Singh, Rishi and Yang, Yalong and Nobre, Carolina and Beyer, Johanna and Smith, Maurice A. and Pfister, Hanspeter},\ntitle = {Towards an Understanding of Situated AR Visualization for Basketball Free-Throw Training},\nyear = {2021},\nisbn = {9781450380966},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3411764.3445649},\ndoi = {10.1145/3411764.3445649},\nabstract = {We present an observational study to compare co-located and situated real-time visualizations in basketball free-throw training. Our goal is to understand the advantages and concerns of applying immersive visualization to real-world skill-based sports training and to provide insights for designing AR sports training systems. We design both a situated 3D visualization on a head-mounted display and a 2D visualization on a co-located display to provide immediate visual feedback on a player\u2019s shot performance. Using a within-subject study design with experienced basketball shooters, we characterize user goals, report on qualitative training experiences, and compare the quantitative training results. Our results show that real-time visual feedback helps athletes refine subsequent shots. Shooters in our study achieve greater angle consistency with our visual feedback. Furthermore, AR visualization promotes an increased focus on body form in athletes. Finally, we present suggestions for the design of future sports AR studies.},\nbooktitle = {Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems},\narticleno = {461},\nnumpages = {13},\nkeywords = {Data Visualization, SportsXR, Augmented Reality, Immersive Analytics, Situated Analytics},\nlocation = {Yokohama, Japan},\nseries = {CHI '21}\n}",
            "DOI": "10.1145/3411764.3445649",
            "Authors": [
                "Lin",
                "Tica and Singh",
                "Rishi and Yang",
                "Yalong and Nobre",
                "Carolina and Beyer",
                "Johanna and Smith",
                "Maurice A. and Pfister",
                "Hanspeter"
            ]
        },
        {
            "Name": "Virtual Reality Sonification Training System Can Improve a Novice's Forehand Return of Serve in Tennis",
            "Opportunity": [
                "Situated Analytics"
            ],
            "Contribution Type": [
                "Technique"
            ],
            "Data Domain": [
                "Sports"
            ],
            "Dataset Types": [
                "Spatial"
            ],
            "Dataset Generation": [
                "Dynamic",
                "Live-computation",
                "Interactive"
            ],
            "Presentation": [
                "HMD",
                "VR"
            ],
            "Device": [
                "HTC Vive pro"
            ],
            "Input": [
                "Custom Controller",
                "Body"
            ],
            "Environment": [
                "Room Scale"
            ],
            "Space": [
                "Finite"
            ],
            "Embodiment": [
                "Exocentric"
            ],
            "Collaboration ": [
                "None"
            ],
            "Interaction": [
                "Move"
            ],
            "Visualization": [
                "3D Trajectory",
                "Area Chart"
            ],
            "Abstract/Natural": [
                "Natural Spatial Mapping"
            ],
            "Manipulate": [
                "Change"
            ],
            "Position": [
                "Fixed (Point)"
            ],
            "Scale": [
                "Large"
            ],
            "2D or 3D": [
                "3D"
            ],
            "Year": "2022",
            "Bibtex": "'@INPROCEEDINGS{9974234,\nauthor={Masai, Katsutoshi and Kajiyama, Takuma and Muramatsu, Tadashi and Sugimoto, Maki and Kimura, Toshitaka},\nbooktitle={2022 IEEE International Symposium on Mixed and Augmented Reality Adjunct (ISMAR-Adjunct)},\ntitle={Virtual Reality Sonification Training System Can Improve a Novice's Forehand Return of Serve in Tennis},\nyear={2022},\nvolume={},\nnumber={},\npages={845-849},\ndoi={10.1109/ISMAR-Adjunct57072.2022.00182}}",
            "DOI": "10.1109/ISMAR-Adjunct57072.2022.00182",
            "Authors": [
                "Masai",
                "Katsutoshi and Kajiyama",
                "Takuma and Muramatsu",
                "Tadashi and Sugimoto",
                "Maki and Kimura",
                "Toshitaka"
            ]
        },
        {
            "Name": "Visual Comparison of Networks in VR",
            "Opportunity": [
                "Spatial Immersion"
            ],
            "Contribution Type": [
                "Evaluation"
            ],
            "Data Domain": [
                "General"
            ],
            "Dataset Types": [
                "Network"
            ],
            "Dataset Generation": [
                "Pre-computation",
                "Static"
            ],
            "Presentation": [
                "VR",
                "HMD"
            ],
            "Device": [
                "Oculus Rift CV1"
            ],
            "Input": [
                "Commercial Controller"
            ],
            "Environment": [
                "Room Scale"
            ],
            "Space": [
                "Finite"
            ],
            "Embodiment": [
                "Exocentric"
            ],
            "Collaboration ": [
                "None"
            ],
            "Interaction": [
                "Raycast",
                "Grab"
            ],
            "Visualization": [
                "3D Node Link",
                "2D Matrix/Heatmap"
            ],
            "Abstract/Natural": [
                "Abstract"
            ],
            "Manipulate": [
                "Translate",
                "Rotate",
                "Select"
            ],
            "Position": [
                "Unfixed"
            ],
            "Scale": [
                "Medium"
            ],
            "2D or 3D": [
                "3D",
                "2D"
            ],
            "Year": "2022",
            "Bibtex": "@ARTICLE{9873980,\n  author={Joos, Lucas and Jaeger-Honz, Sabrina and Schreiber, Falk and Keim, Daniel A. and Klein, Karsten},\n  journal={IEEE Transactions on Visualization and Computer Graphics}, \n  title={Visual Comparison of Networks in VR}, \n  year={2022},\n  volume={28},\n  number={11},\n  pages={3651-3661},\n  doi={10.1109/TVCG.2022.3203001}}",
            "DOI": "10.1109/TVCG.2022.3203001",
            "Authors": [
                "Joos",
                "Lucas and Jaeger-Honz",
                "Sabrina and Schreiber",
                "Falk and Keim",
                "Daniel A. and Klein",
                "Karsten"
            ]
        },
        {
            "Name": "Visualizing Natural Environments from Data in Virtual Reality: Combining Realism and Uncertainty",
            "Opportunity": [
                "Engagement"
            ],
            "Contribution Type": [
                "Technique"
            ],
            "Data Domain": [
                "Environmental",
                "Life sciences"
            ],
            "Dataset Types": [
                "Tabular"
            ],
            "Dataset Generation": [
                "Static"
            ],
            "Presentation": [
                "HMD",
                "VR"
            ],
            "Device": [
                "HTC vive"
            ],
            "Input": [
                "Commercial Controller"
            ],
            "Environment": [
                "Room Scale"
            ],
            "Space": [
                "Infinite"
            ],
            "Embodiment": [
                "Egocentric"
            ],
            "Collaboration ": [
                "None"
            ],
            "Interaction": [
                "Raycast",
                "Move"
            ],
            "Visualization": [
                "Glyph/Realism"
            ],
            "Abstract/Natural": [
                "Abstract"
            ],
            "Manipulate": [
                "Navigate",
                "Change",
                "Filter"
            ],
            "Position": [
                "Unfixed"
            ],
            "Scale": [
                "Large"
            ],
            "2D or 3D": [
                "3D"
            ],
            "Year": "2019",
            "Bibtex": "'@INPROCEEDINGS{8797996,\n  author={Huang, Jiawei and Lucash, Melissa S. and Simpson, Mark B. and Helgeson, Casey and Klippel, Alexander},\n  booktitle={2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, \n  title={Visualizing Natural Environments from Data in Virtual Reality: Combining Realism and Uncertainty}, \n  year={2019},\n  volume={},\n  number={},\n  pages={1485-1488},\n  doi={10.1109/VR.2019.8797996}}",
            "DOI": "10.1109/VR.2019.8797996",
            "Authors": [
                "J. Huang; M. S. Lucash; M. B. Simpson; C. Helgeson; A. Klippel"
            ]
        },
        {
            "Name": "Visualizing Planetary Spectroscopy through Immersive On-site Rendering",
            "Opportunity": [
                "Spatial Immersion"
            ],
            "Contribution Type": [
                "System"
            ],
            "Data Domain": [
                "Astronomy",
                "Physical Sciences"
            ],
            "Dataset Types": [
                "Spatial",
                "Tabular"
            ],
            "Dataset Generation": [
                "Static",
                "Pre-computation"
            ],
            "Presentation": [
                "VR",
                "HMD"
            ],
            "Device": [
                "Oculus Quest"
            ],
            "Input": [
                "Commercial Controller"
            ],
            "Environment": [
                "Room Scale"
            ],
            "Space": [
                "Infinite"
            ],
            "Embodiment": [
                "Exocentric"
            ],
            "Collaboration ": [
                "None"
            ],
            "Interaction": [
                "Raycast",
                "Gaze"
            ],
            "Visualization": [
                "2D Line Chart",
                "2D Bar Chart",
                "3D Area Chart",
                "3D Map"
            ],
            "Abstract/Natural": [
                "Abstract"
            ],
            "Manipulate": [
                "Zoom",
                "Select",
                "Navigate"
            ],
            "Position": [
                "Fixed (Point)",
                "Fixed (Body)"
            ],
            "Scale": [
                "Small",
                "Large"
            ],
            "2D or 3D": [
                "2D",
                "3D"
            ],
            "Year": "2021",
            "Bibtex": "'@INPROCEEDINGS{9417645,\n  author={Gold, Lauren and Bahremand, Alireza and Richards, Connor and Hertzberg, Justin and Sese, Kyle and Gonzalez, Alexander and Purcell, Zoe and Powell, Kathryn and LiKamWa, Robert},\n  booktitle={2021 IEEE Virtual Reality and 3D User Interfaces (VR)}, \n  title={Visualizing Planetary Spectroscopy through Immersive On-site Rendering}, \n  year={2021},\n  volume={},\n  number={},\n  pages={428-437},\n  doi={10.1109/VR50410.2021.00066}}",
            "DOI": "10.1109/VR50410.2021.00066",
            "Authors": [
                "L. Gold; A. Bahremand; C. Richards; J. Hertzberg; K. Sese; A. Gonzalez; Z. Purcell; K. Powell; R. LiKamWa"
            ]
        },
        {
            "Name": "Vivern\u2013A Virtual Environment for Multiscale Visualization and Modeling of DNA Nanostructures",
            "Opportunity": [
                "Embodied Data Exploration"
            ],
            "Contribution Type": [
                "Design Study"
            ],
            "Data Domain": [
                "Biology",
                "Life sciences"
            ],
            "Dataset Types": [
                "Tabular"
            ],
            "Dataset Generation": [
                "Pre-computation",
                "Static"
            ],
            "Presentation": [
                "VR",
                "HMD"
            ],
            "Device": [
                "Oculus Rift S",
                "HTC vive"
            ],
            "Input": [
                "Commercial Controller"
            ],
            "Environment": [
                "Room Scale"
            ],
            "Space": [
                "Finite"
            ],
            "Embodiment": [
                "Exocentric"
            ],
            "Collaboration ": [
                "None"
            ],
            "Interaction": [
                "Grab",
                "Rotate",
                "Click",
                "Move"
            ],
            "Visualization": [
                "Glyph/Realism"
            ],
            "Abstract/Natural": [
                "Abstract"
            ],
            "Manipulate": [
                "Arrange",
                "Translate",
                "Change",
                "Pan",
                "Slice",
                "Select"
            ],
            "Position": [
                "Unfixed"
            ],
            "Scale": [
                "Large"
            ],
            "2D or 3D": [
                "3D"
            ],
            "Year": "2022",
            "Bibtex": "@ARTICLE{9523759,\n  author={Ku\u0165\u00e1k, David and Selzer, Matias Nicol\u00e1s and By\u0161ka, Jan and Ganuza, Mar\u00eda Luj\u00e1n and Bari\u0161i\u0107, Ivan and Kozl\u00edkov\u00e1, Barbora and Miao, Haichao},\n  journal={IEEE Transactions on Visualization and Computer Graphics}, \n  title={Vivern\u2013A Virtual Environment for Multiscale Visualization and Modeling of DNA Nanostructures}, \n  year={2022},\n  volume={28},\n  number={12},\n  pages={4825-4838},\n  doi={10.1109/TVCG.2021.3106328}}",
            "DOI": "10.1109/TVCG.2021.3106328",
            "Authors": [
                "Ku\u0165\u00e1k",
                "David and Selzer",
                "Matias Nicol\u00e1s and By\u0161ka",
                "Jan and Ganuza",
                "Mar\u00eda Luj\u00e1n and Bari\u0161i\u0107",
                "Ivan and Kozl\u00edkov\u00e1",
                "Barbora and Miao",
                "Haichao"
            ]
        },
        {
            "Name": "VRGaitAnalytics: Visualizing Dual Task Cost for VR Gait Assessment",
            "Opportunity": [
                "Spatial Immersion"
            ],
            "Contribution Type": [
                "System"
            ],
            "Data Domain": [
                "Physical Sciences",
                "Health",
                "Life sciences"
            ],
            "Dataset Types": [
                "Spatial"
            ],
            "Dataset Generation": [
                "Live-computation"
            ],
            "Presentation": [
                "HMD",
                "VR"
            ],
            "Device": [
                "HTC Vive pro"
            ],
            "Input": [
                "Body"
            ],
            "Environment": [
                "Room Scale"
            ],
            "Space": [
                "Infinite"
            ],
            "Embodiment": [
                "Exocentric"
            ],
            "Collaboration ": [
                "None"
            ],
            "Interaction": [
                "Move"
            ],
            "Visualization": [
                "2D Line Chart",
                "3D Trajectory"
            ],
            "Abstract/Natural": [
                "Abstract"
            ],
            "Manipulate": [
                "Change",
                "Arrange"
            ],
            "Position": [
                "Unfixed",
                "Fixed (Body)"
            ],
            "Scale": [
                "Medium"
            ],
            "2D or 3D": [
                "2D",
                "3D"
            ],
            "Year": "2021",
            "Bibtex": "'@inproceedings{10.1145/3489849.3489874,\nauthor = {Wang, Zhu and Arie, Liraz and Lubetzky, Anat V. and Perlin, Ken},\ntitle = {VRGaitAnalytics: Visualizing Dual Task Cost for VR Gait Assessment},\nyear = {2021},\nisbn = {9781450390927},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3489849.3489874},\ndoi = {10.1145/3489849.3489874},\nabstract = {Among its many promising applications, Virtual Reality (VR) can simulate diverse real-life scenarios and therefore help experimenters assess individuals\u2019 gait performance (i.e., walking) under controlled functional contexts. VR-based gait assessment may provide low-risk, reproducible and controlled virtual environments, enabling experimenters to investigate underlying causes for imbalance by manipulating experimental conditions such as multi-sensory loads, mental processing loads (cognitive load), and/or motor tasks. We present a low-cost novel VR gait assessment system that simulates virtual obstacles, visual, auditory, and cognitive loads while using motion tracking to assess participants\u2019 walking performance. The system utilizes in-situ spatial visualization for trial playback and instantaneous outcome measures which enable experimenters and participants to observe and interpret their performance. The trial playback can visualize any moment in the trial with embodied graphic segments including the head, waist, and feet. It can also replay two trials at the same time frame for trial-to-trial comparison, which helps visualize the impact of different experimental conditions. The outcome measures, i.e., the metrics related to walking performance, are calculated in real-time and displayed as data graphs in VR. The system can help experimenters get specific gait information on balance performance beyond a typical clinical gait test, making it clinically relevant and potentially applicable to gait rehabilitation. We conducted a feasibility study with physical therapy students, research graduate students, and licensed physical therapists. They evaluated the system and provided feedback on the outcome measures, the spatial visualizations, and the potential use of the system in the clinic. The study results indicate that the system was feasible for gait assessment, and the immediate spatial visualization features were seen as clinically relevant and useful. Limitations and considerations for future work are discussed.},\nbooktitle = {Proceedings of the 27th ACM Symposium on Virtual Reality Software and Technology},\narticleno = {15},\nnumpages = {10},\nkeywords = {gait balance, obstacle crossing, playback, Spatial visualization, virtual reality},\nlocation = {Osaka, Japan},\nseries = {VRST '21}\n}",
            "DOI": "10.1145/3489849.3489874",
            "Authors": [
                "Wang",
                "Zhu and Arie",
                "Liraz and Lubetzky",
                "Anat V. and Perlin",
                "Ken"
            ]
        },
        {
            "Name": "WA VIS: A Web-Based Augmented Reality Text Data Visual Analysis Tool",
            "Opportunity": [
                "Situated Analytics"
            ],
            "Contribution Type": [
                "Technique"
            ],
            "Data Domain": [
                "Literature"
            ],
            "Dataset Types": [
                "Tabular"
            ],
            "Dataset Generation": [
                "Live-computation"
            ],
            "Presentation": [
                "AR",
                "Phone",
                "Handheld"
            ],
            "Device": [
                "Nubia z17mini"
            ],
            "Input": [
                "Gesture",
                "Mobile Device"
            ],
            "Environment": [
                "Seated",
                "Table",
                "Object"
            ],
            "Space": [
                "Situated"
            ],
            "Embodiment": [
                "Egocentric"
            ],
            "Collaboration ": [
                "None"
            ],
            "Interaction": [
                "Touch",
                "Pinch",
                "Move",
                "Rotate"
            ],
            "Visualization": [
                "2D Word Cloud",
                "2D Pie Chart",
                "2D Storyline"
            ],
            "Abstract/Natural": [
                "Abstract"
            ],
            "Manipulate": [
                "Navigate",
                "Select",
                "Arrange"
            ],
            "Position": [
                "Fixed (Point)",
                "Fixed (Object)"
            ],
            "Scale": [
                "Small"
            ],
            "2D or 3D": [
                "2D"
            ],
            "Year": "2019",
            "Bibtex": "'@INPROCEEDINGS{9212857,\n  author={Pei, Yunqiang and Wu, Yadong and Wang, Song and Wang, Fupan and Jiang, Hongyu and Xu, Shijian and Zhou, Jinquan},\n  booktitle={2019 International Conference on Virtual Reality and Visualization (ICVRV)}, \n  title={WA VIS: A Web-Based Augmented Reality Text Data Visual Analysis Tool}, \n  year={2019},\n  volume={},\n  number={},\n  pages={11-17},\n  doi={10.1109/ICVRV47840.2019.00011}}",
            "DOI": "10.1109/ICVRV47840.2019.00011",
            "Authors": [
                "Y. Pei; Y. Wu; S. Wang; F. Wang; H. Jiang; S. Xu; J. Zhou"
            ]
        },
        {
            "Name": "Worlds-in-Wedges: Combining Worlds-in-Miniature and Portals to Support Comparative Immersive Visualization of Forestry Data",
            "Opportunity": [
                "Spatial Immersion"
            ],
            "Contribution Type": [
                "Technique"
            ],
            "Data Domain": [
                "Ecology",
                "Life sciences"
            ],
            "Dataset Types": [
                "Tabular"
            ],
            "Dataset Generation": [
                "Static",
                "Pre-computation"
            ],
            "Presentation": [
                "VR",
                "HMD"
            ],
            "Device": [
                "HTC vive"
            ],
            "Input": [
                "Commercial Controller"
            ],
            "Environment": [
                "Room Scale"
            ],
            "Space": [
                "Infinite"
            ],
            "Embodiment": [
                "Egocentric"
            ],
            "Collaboration ": [
                "None"
            ],
            "Interaction": [
                "Raycast",
                "Grab",
                "Rotate",
                "Move"
            ],
            "Visualization": [
                "Glyph/Realism",
                "3D Map"
            ],
            "Abstract/Natural": [
                "Abstract"
            ],
            "Manipulate": [
                "Select",
                "Zoom",
                "Navigate",
                "Rotate",
                "Arrange",
                "Pan"
            ],
            "Position": [
                "Unfixed"
            ],
            "Scale": [
                "Large",
                "Medium"
            ],
            "2D or 3D": [
                "3D"
            ],
            "Year": "2019",
            "Bibtex": "'@INPROCEEDINGS{8797871,\n  author={Nam, Jung Who and McCullough, Krista and Tveite, Joshua and Espinosa, Maria Molina and Perry, Charles H. and Wilson, Barry T. and Keefe, Daniel F.},\n  booktitle={2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, \n  title={Worlds-in-Wedges: Combining Worlds-in-Miniature and Portals to Support Comparative Immersive Visualization of Forestry Data}, \n  year={2019},\n  volume={},\n  number={},\n  pages={747-755},\n  doi={10.1109/VR.2019.8797871}}",
            "DOI": "10.1109/VR.2019.8797871",
            "Authors": [
                "J. W. Nam; K. McCullough; J. Tveite; M. M. Espinosa; C. H. Perry; B. T. Wilson; D. F. Keefe"
            ]
        },
        {
            "Name": "X-Space: A Tool for Extending Mixed Reality Space from Web2D Visualization Anywhere",
            "Opportunity": [
                "Spatial Immersion"
            ],
            "Contribution Type": [
                "System"
            ],
            "Data Domain": [
                "General"
            ],
            "Dataset Types": [
                "Tabular"
            ],
            "Dataset Generation": [
                "Dynamic",
                "Live-computation"
            ],
            "Presentation": [
                "HMD",
                "AR",
                "Desktop"
            ],
            "Device": [
                "Hololens"
            ],
            "Input": [
                "Gesture",
                "Mobile Device",
                "Mouse/Keyboard"
            ],
            "Environment": [
                "Seated"
            ],
            "Space": [
                "Finite"
            ],
            "Embodiment": [
                "Egocentric"
            ],
            "Collaboration ": [
                "None"
            ],
            "Interaction": [
                "Touch",
                "Pinch",
                "Move"
            ],
            "Visualization": [
                "2D Bar Chart",
                "2D Line Chart",
                "2D Scatter Plot",
                "3D Bar Chart",
                "3D Scatter Plot",
                "3D Word Cloud",
                "3D Matrix/Heatmap"
            ],
            "Abstract/Natural": [
                "Abstract"
            ],
            "Manipulate": [
                "Select",
                "Change",
                "Filter"
            ],
            "Position": [
                "Unfixed"
            ],
            "Scale": [
                "Medium",
                "Small"
            ],
            "2D or 3D": [
                "2D",
                "3D"
            ],
            "Year": "2022",
            "Bibtex": "'@INPROCEEDINGS{9974439,\n  author={Li, Tiemeng and Wu, Songqian and Jin, Yanning and Shi, Haopai and Liu, Shiran},\n  booktitle={2022 IEEE International Symposium on Mixed and Augmented Reality Adjunct (ISMAR-Adjunct)}, \n  title={X-Space: A Tool for Extending Mixed Reality Space from Web2D Visualization Anywhere}, \n  year={2022},\n  volume={},\n  number={},\n  pages={124-130},\n  doi={10.1109/ISMAR-Adjunct57072.2022.00032}}",
            "DOI": "10.1109/ISMAR-Adjunct57072.2022.00032",
            "Authors": [
                "Tiemeng Li; Songqian Wu; Yanning Jin; Haopai Shi; Shiran Liu"
            ]
        },
        {
            "Name": "XVCollab: An Immersive Analytics Tool for Asymmetric Collaboration across the Virtuality Spectrum",
            "Opportunity": [
                "Collaboration"
            ],
            "Contribution Type": [
                "System"
            ],
            "Data Domain": [
                "General"
            ],
            "Dataset Types": [
                "Tabular"
            ],
            "Dataset Generation": [
                "Static",
                "Pre-computation"
            ],
            "Presentation": [
                "HMD",
                "VR",
                "Desktop",
                "AR"
            ],
            "Device": [
                "HTC vive"
            ],
            "Input": [
                "Commercial Controller",
                "Gesture",
                "Mouse/Keyboard"
            ],
            "Environment": [
                "Seated",
                "Room Scale"
            ],
            "Space": [
                "Finite"
            ],
            "Embodiment": [
                "Egocentric",
                "Exocentric"
            ],
            "Collaboration ": [
                "Synchronous Co-located",
                "Cross-virtuality"
            ],
            "Interaction": [
                "Grab",
                "Raycast",
                "Click"
            ],
            "Visualization": [
                "2D Bar Chart",
                "2D Scatter Plot",
                "3D Scatter Plot",
                "Parallel Coord",
                "3D Parallel Coord",
                "2D Line Chart"
            ],
            "Abstract/Natural": [
                "Abstract"
            ],
            "Manipulate": [
                "Filter",
                "Change",
                "Rotate",
                "Arrange",
                "Translate"
            ],
            "Position": [
                "Unfixed"
            ],
            "Scale": [
                "Medium"
            ],
            "2D or 3D": [
                "2D",
                "3D"
            ],
            "Year": "2022",
            "Bibtex": "'@INPROCEEDINGS{9974396,\nauthor={Seraji, Mohammad Rajabi and Stuerzlinger, Wolfgang},\nbooktitle={2022 IEEE International Symposium on Mixed and Augmented Reality Adjunct (ISMAR-Adjunct)},\ntitle={XVCollab: An Immersive Analytics Tool for Asymmetric Collaboration across the Virtuality Spectrum},\nyear={2022},\nvolume={},\nnumber={},\npages={146-154},\ndoi={10.1109/ISMAR-Adjunct57072.2022.00035}}",
            "DOI": "10.1109/ISMAR-Adjunct57072.2022.00035",
            "Authors": [
                "Seraji",
                "Mohammad Rajabi and Stuerzlinger",
                "Wolfgang"
            ]
        }
    ]
}